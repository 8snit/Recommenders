{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Databricks as a Compute Target from Azure Machine Learning Pipeline\n",
    "To use Databricks as a compute target from [Azure Machine Learning Pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines), a [DatabricksStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.databricks_step.databricksstep?view=azure-ml-py) is used. This notebook demonstrates the use of DatabricksStep in Azure Machine Learning Pipeline.\n",
    "\n",
    "The notebook will show:\n",
    "1. Running an arbitrary Databricks notebook that the customer has in Databricks workspace\n",
    "2. Running an arbitrary Python script that the customer has in DBFS\n",
    "3. Running an arbitrary Python script that is available on local computer (will upload to DBFS, and then run in Databricks) \n",
    "\n",
    "\n",
    "## Before you begin:\n",
    "\n",
    "1. **Create an Azure Databricks workspace** in the same subscription where you have your Azure Machine Learning workspace. You will need details of this workspace later on to define DatabricksStep. [Click here](https://ms.portal.azure.com/#blade/HubsExtension/Resources/resourceType/Microsoft.Databricks%2Fworkspaces) for more information.\n",
    "2. **Create PAT (access token)**: Manually create a Databricks access token at the Azure Databricks portal. See [this](https://docs.databricks.com/api/latest/authentication.html#generate-a-token) for more information.\n",
    "3. **Add demo notebook to ADB**: This notebook has a sample you can use as is. Launch Azure Databricks attached to your Azure Machine Learning workspace and add a new notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of AML workspace\n",
    "\n",
    "Replace the default values in the cell below with your AML workspace parameters. For details of setting up AML workspace, please [click here](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"***\") #subscription id\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"AMLtestye\") #AML resource group\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"testamlye711\") #name of AML workspace\n",
    "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"southcentralus\")  ## AML workspace region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access your workspace\n",
    "The following cell uses the Azure ML SDK to attempt to load the workspace specified by your parameters. If this cell succeeds, your notebook library will be configured to access the workspace from all notebooks using the Workspace.from_config() method. The cell can fail if the specified workspace doesn't exist or you don't have permissions to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote the config file config.json to: /data/home/adminye/notebooks/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml_config/config.json\n",
      "Workspace configuration succeeded. Skip the workspace creation steps below\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    # write the details of the workspace to a configuration file to the notebook library\n",
    "    ws.write_config()\n",
    "    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\n",
    "except:\n",
    "    print(\"Workspace not accessible. Change your parameters or create a new workspace below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.runconfig import JarLibrary\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from msrest.exceptions import HttpOperationError\n",
    "from azureml.core.runconfig import RunConfiguration, EggLibrary,PyPiLibrary\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. Make sure the config file is present at .\\config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /data/home/adminye/notebooks/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml_config/config.json\n",
      "testamlye711\n",
      "AMLtestye\n",
      "southcentralus\n",
      "03909a66-bef8-4d52-8e9a-a346604e0902\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Databricks compute target\n",
    "Next, you need to add your Databricks workspace to Azure Machine Learning as a compute target and give it a name. You will use this name to refer to your Databricks workspace compute target inside Azure Machine Learning.\n",
    "\n",
    "- **Resource Group** - The resource group name of your Azure Machine Learning workspace\n",
    "- **Databricks Workspace Name** - The workspace name of your Azure Databricks workspace\n",
    "- **Databricks Access Token** - The access token you created in ADB\n",
    "\n",
    "**The Databricks workspace need to be present in the same subscription as your AML workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute target testye already exists\n"
     ]
    }
   ],
   "source": [
    "# Replace with your account info before running.\n",
    " \n",
    "db_compute_name=os.getenv(\"DATABRICKS_COMPUTE_NAME\", \"testye\") # Databricks compute name 'testye' aml compute\n",
    "db_resource_group=os.getenv(\"DATABRICKS_RESOURCE_GROUP\", \"AMLtestye2\") # Databricks resource group\n",
    "db_workspace_name=os.getenv(\"DATABRICKS_WORKSPACE_NAME\", \"testAMLye\") # Databricks workspace name\n",
    "db_access_token=os.getenv(\"DATABRICKS\",\"***\") # Databricks access token\n",
    " \n",
    "try:\n",
    "    databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
    "    print('Compute target {} already exists'.format(db_compute_name))\n",
    "except ComputeTargetException:\n",
    "    print('Compute not found, will use below parameters to attach new one')\n",
    "    print('db_compute_name {}'.format(db_compute_name))\n",
    "    print('db_resource_group {}'.format(db_resource_group))\n",
    "    print('db_workspace_name {}'.format(db_workspace_name))\n",
    "    print('db_access_token {}'.format(db_access_token))\n",
    " \n",
    "    config = DatabricksCompute.attach_configuration(\n",
    "        resource_group = db_resource_group,\n",
    "        workspace_name = db_workspace_name,\n",
    "        access_token= db_access_token)\n",
    "    databricks_compute=ComputeTarget.attach(ws, db_compute_name, config)\n",
    "    databricks_compute.wait_for_completion(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Databricks from Azure Machine Learning Pipeline\n",
    "To use Databricks as a compute target from Azure Machine Learning Pipeline, a DatabricksStep is used. Let's define a datasource (via DataReference) and intermediate data (via PipelineData) to be used in DatabricksStep. This section creates a new Azure DB job clusters to run a ALS deep dive script on local computer/Azure DB workspace/DBFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the code below is not used for this demo but would be used in the future if you want to load data from blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "blob_datastore_name='MyBlobDatastore'\n",
    "\n",
    "account_name=os.getenv(\"BLOB_ACCOUNTNAME_62\", \"amltestyediag\") # Storage account name\n",
    "container_name=os.getenv(\"BLOB_CONTAINER_62\", \"movielens\") # Name of Azure blob container\n",
    "account_key=os.getenv(\"BLOB_ACCOUNT_KEY_62\", \"***\") # Storage account key\n",
    "\n",
    "##connect data in blob to Datastore\n",
    "try:\n",
    "    blob_datastore = Datastore.get(ws, blob_datastore_name)\n",
    "    print(\"found blob datastore with name: %s\" % blob_datastore_name)\n",
    "except HttpOperationError:\n",
    "    blob_datastore = Datastore.register_azure_blob_container(\n",
    "        workspace=ws,\n",
    "        datastore_name=blob_datastore_name,\n",
    "        account_name=account_name, # Storage account name\n",
    "        container_name=container_name, # Name of Azure blob container\n",
    "        account_key=account_key) # Storage account key\"\n",
    "    print(\"registered blob datastore with name: %s\" % blob_datastore_name)\n",
    "print('Datastore {} will be used'.format(blob_datastore.name))\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default blob storage\n",
    "#def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "#print('Datastore {} will be used'.format(def_blob_store.name))\n",
    "\n",
    "# We are uploading a sample file in the local directory to be used as a datasource\n",
    "#def_blob_store.upload_files(files=[\"./testdata.txt\"], target_path=\"dbtest\", overwrite=False)\n",
    "\n",
    "step_1_input = DataReference(datastore=blob_datastore, path_on_datastore=\"movielens\",\n",
    "                                     data_reference_name=\"input\")\n",
    "\n",
    "step_1_output = PipelineData(\"output\", datastore=blob_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_datastore = Datastore.get(ws, blob_datastore_name)\n",
    "print(\"found blob datastore with name: %s\" % blob_datastore_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Running the demo notebook already added to the Databricks workspace\n",
    "Upload the training notebook in the Azure Databricks workspace, and provide the path to that notebook as the value associated with the environment variable \"DATABRICKS_NOTEBOOK_PATH\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "notebook_path=os.getenv(\"DATABRICKS_NOTEBOOK_PATH\", \"/Users/yexing@microsoft.com/als_deep_dive\") # Databricks notebook path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To create the compute environment for a ADB cluster, there are two options:\n",
    "### 1.a Create a runconfig file which contains the required libraries, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a runconfig file which contains the required libraries\n",
    "\n",
    "## set up the RunConfiguration\n",
    "adb_runconfig = RunConfiguration()\n",
    "adb_runconfig.target = databricks_compute.name\n",
    "adb_runconfig.environment.databricks.egg_libraries=[EggLibrary(library=\"dbfs:/FileStore/jars/a7c31364_6a02_4e76_814e_002c06a31b1a-Recommenders.egg\")]\n",
    "adb_runconfig.environment.databricks.pypi_libraries=[PyPiLibrary(package=\"azureml-sdk[databricks]\", repo=None)]\n",
    "#adb_runconfig.environment.databricks.jar_libraries=[JarLibrary(library='dbfs:/FileStore/jars/azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar')]\n",
    "## this will save to ./aml_config/adb_compute.runconfig\n",
    "## *IF* ./aml_config exists\n",
    "adb_runconfig.save(path='.', name='adb_compute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration, EggLibrary,PyPiLibrary\n",
    "\n",
    "notebook_path=os.getenv(\"DATABRICKS_NOTEBOOK_PATH\", \"/Users/yexing@microsoft.com/als_deep_dive\") # Databricks notebook path\n",
    "\n",
    "dbNbStep = DatabricksStep(\n",
    "    name=\"DBNotebookInWS\", #name of the step\n",
    "    num_workers=1,\n",
    "    notebook_path=notebook_path,\n",
    "    #inputs=[step_1_input],\n",
    "    #outputs=[step_1_output],\n",
    "    #notebook_params={'myparam': 'testparam'}, we didn't pass the input/parameters in this example\n",
    "    run_name='DB_Notebook_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    runconfig=adb_runconfig,\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Specify the egg_libraries and pypilibrary in DatabrickStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbNbStep = DatabricksStep(\n",
    "    name=\"DBNotebookInWS\", #name of the step\n",
    "    num_workers=1,\n",
    "    notebook_path=notebook_path,\n",
    "    run_name='DB_Notebook_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    egg_libraries=[EggLibrary(library=\"dbfs:/FileStore/jars/a7c31364_6a02_4e76_814e_002c06a31b1a-Recommenders.egg\")],\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and submit the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step DBNotebookInWS [b3fda6b9][bf6cac1a-55fc-4eda-b646-7d28e06df95d], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: 8a7a333a-d346-4c9c-a8da-4230b59d0eb8\n",
      "status:Running\n",
      "....................................................................................................................\n",
      "status:Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [dbNbStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_Notebook_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26b4e0dc55043a4a8ff640d3763f235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSET'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running a Python script that is already added in DBFS\n",
    "To run a Python script that is already uploaded to DBFS, follow the instructions below. You will first upload the Python script to DBFS using the [CLI](https://docs.azuredatabricks.net/user-guide/dbfs-databricks-file-system.html).\n",
    "\n",
    "The commented out code in the below cell assumes that you have uploaded `als_deep_dive.py` to the root folder in DBFS. You can upload `als_deep_dive.py` to the folder \"scripts\" in DBFS using this commandline so you can use `python_script_path = \"dbfs:/scripts/train-db-dbfs.py\"`:\n",
    "\n",
    "```\n",
    "dbfs mkdirs dbfs:/scripts\n",
    "dbfs cp ./als_deep_dive.py dbfs:/scripts/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_path = \"dbfs:/scripts/als_deep_dive.py\"\n",
    "\n",
    "dbPythonInDbfsStep = DatabricksStep(\n",
    "    name=\"DBPythonInDBFS\",\n",
    "    num_workers=1,\n",
    "    python_script_path=python_script_path,\n",
    "    run_name='DB_Python_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    egg_libraries=[EggLibrary(library=\"dbfs:/FileStore/jars/a7c31364_6a02_4e76_814e_002c06a31b1a-Recommenders.egg\")],\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and submit the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step DBPythonInDBFS [7542990c][f8ea50f7-4a17-467c-8eab-9b265ecd5383], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: 38c551ef-3de6-4830-8f70-2b3059ee069d\n",
      "status:Running\n",
      "...........................................................................................................................\n",
      "status:Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [dbPythonInDbfsStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_Python_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507debe9396b434cbf9f95e0fa017235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSET'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running a Python script in Databricks that currenlty is in local computer\n",
    "To run a Python script that is currently in your local computer, follow the instructions below. \n",
    "\n",
    "The commented out code below code assumes that you have `als_deep_dive.py` in the `scripts` subdirectory under the current working directory.\n",
    "\n",
    "In this case, the Python script will be uploaded first to DBFS, and then the script will be run in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_name = \"als_deep_dive.py\"\n",
    "source_directory = \".\"  \n",
    "\n",
    "dbPythonInLocalMachineStep = DatabricksStep(\n",
    "    name=\"DBPythonInLocalMachine\",\n",
    "    num_workers=1,\n",
    "    python_script_name=python_script_name,\n",
    "    source_directory=source_directory,\n",
    "    run_name='DB_Python_Local_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    egg_libraries=[EggLibrary(library=\"dbfs:/FileStore/jars/a7c31364_6a02_4e76_814e_002c06a31b1a-Recommenders.egg\")],\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and submit the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step DBPythonInLocalMachine [c567528e][e7d00e1d-3689-4e33-a131-837504372cbe], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: 27213cdb-ce4d-43a4-8a7c-d1b02c0d209a\n",
      "status:Running\n",
      "....................................................................................................................................\n",
      "status:Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [dbPythonInLocalMachineStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_Python_Local_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e177f3e180ae4613b7c18aa4f1c9e1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSET'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "diray"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
