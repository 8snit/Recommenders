{"cells":[{"cell_type":"markdown","source":["Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}},{"cell_type":"code","source":["import os\nimport json\nimport shutil"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["from reco_utils.dataset.criteo import get_spark_schema, load_spark_df\n\nfrom azureml.core import Workspace\nfrom azureml.core import VERSION as azureml_version\n\nfrom azureml.core.model import Model\nfrom azureml.core.conda_dependencies import CondaDependencies \nfrom azureml.core.webservice import Webservice, AksWebservice\nfrom azureml.core.image import ContainerImage\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# Check core SDK version number\nprint(\"Azure ML SDK version: {}\".format(azureml_version))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Azure ML SDK version: 1.0.8\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## Configure Scoring Service Variables"],"metadata":{}},{"cell_type":"code","source":["MODEL_NAME = \"criteo-lgbm.model\"  # this name must exactly match the name used to save the pipeline model in the estimation notebook\nCONDA_FILE = \"deploy_conda.yaml\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## Create the conda dependencies file"],"metadata":{}},{"cell_type":"code","source":["# azureml-sdk is required to load the registered model\nconda_file = CondaDependencies.create(pip_packages=['azureml-sdk', 'requests']).serialize_to_string()\n\nwith open(CONDA_FILE, \"w\") as f:\n    f.write(conda_file)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## Define the Scoring Script\n\nNext we, need to create the driver script that will be executed when the service is called. The functions that need to be defined for scoring are `init()` and `run()`. The `init()` function is run when the service is created, and the `run()` function is run each time the service is called.\n\nIn our example, we use the `init()` function to load all the libraries, initialize the spark session, start the spark streaming service and load the model pipeline. We use the `run()` method to route the input to the spark streaming service to generate predictions (in this case the probability of an interaction) then return the output."],"metadata":{}},{"cell_type":"markdown","source":["## Version 1 - LightGBM with streaming"],"metadata":{}},{"cell_type":"code","source":["DRIVER_FILE = \"mmlspark_streamscore.py\"\n\ndriver_file = '''\nimport os\nimport json\nfrom time import sleep\nfrom uuid import uuid4\nfrom zipfile import ZipFile\n\nfrom pyspark.ml import PipelineModel\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType\nimport requests\n\n\ndef init():\n    \"\"\"One time initialization of pyspark and model server\"\"\"\n\n    spark = SparkSession.builder.appName(\"Model Server\").getOrCreate()\n    import mmlspark  # this is needed to load mmlspark libraries\n\n    # extract and load model\n    model_path = Model.get_model_path('{model_name}')\n    with ZipFile(model_path, 'r') as f:\n        f.extractall('model')\n    model = PipelineModel.load('model')\n\n    # load data schema saved with model\n    with open(os.path.join('model', 'schema.json'), 'r') as f:\n        schema = StructType.fromJson(json.load(f))\n\n    input_df = (\n        spark.readStream.continuousServer()\n        .address(\"localhost\", 8089, \"predict\")\n        .load()\n        .parseRequest(schema)\n    )\n\n    output_df = (\n        model.transform(input_df)\n        .makeReply(\"probability\")\n    )\n\n    checkpoint = os.path.join('/tmp', 'checkpoints', uuid4().hex)\n    server = (\n        output_df.writeStream.continuousServer()\n        .trigger(continuous=\"1 second\")\n        .replyTo(\"predict\")\n        .queryName(\"prediction\")\n        .option(\"checkpointLocation\", checkpoint)\n        .start()\n    )\n\n    # let the server finish starting\n    sleep(1)\n\n\ndef run(input_json):\n    try:\n        response = requests.post(data=input_json, url='http://localhost:8089/predict')\n        result = response.json()['probability']['values'][1]\n    except Exception as e:\n        result = str(e)\n    \n    return json.dumps({{\"result\": result}})\n    \n'''.format(model_name=MODEL_NAME)\n\n# check syntax\nexec(driver_file)\n\nwith open(DRIVER_FILE, \"w\") as f:\n    f.write(driver_file)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["## Version 2 - LightGBM - base"],"metadata":{}},{"cell_type":"code","source":["DRIVER_FILE = \"mmlspark_basescore.py\"\n\nscore_sparkml = \"\"\"\n\nimport json\nfrom zipfile import ZipFile\n\n \ndef init():\n    # One-time initialization of PySpark and predictive model\n    import pyspark\n    from pyspark.ml import PipelineModel\n    from mmlspark import LightGBMClassifier\n    from azureml.core.model import Model\n    from pyspark.ml import PipelineModel\n    from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n    global trainedModel\n    global spark\n    global schema\n\n    spark = pyspark.sql.SparkSession.builder.appName(\"LightGBM Criteo Predictions\").getOrCreate()\n    model_path = Model.get_model_path('{model_name}')\n    with ZipFile(model_path, 'r') as f:\n        f.extractall('model')\n    trainedModel = PipelineModel.load('model')\n    \ndef run(input_json):\n    if isinstance(trainedModel, Exception):\n        return json.dumps({{\"trainedModel\":str(trainedModel)}})\n      \n    try:\n        sc = spark.sparkContext\n        input_list = json.loads(input_json)\n        input_rdd = sc.parallelize(input_list)\n        input_df = spark.read.json(input_rdd)\n        \n        # Compute prediction\n        predictions = trainedModel.transform(input_df).collect()\n        #Get probability of a click for each row and conver to a str\n        click_prob = [str(x.probability[1]) for x in predictions]\n\n        # you can return any data type as long as it is JSON-serializable\n        result = \",\".join(click_prob)\n        return [result]\n    except Exception as e:\n        result = str(e)\n        return result\n\"\"\".format(model_name=MODEL_NAME)\n \nexec(score_sparkml)\n \nwith open(DRIVER_FILE, \"w\") as file:\n    file.write(score_sparkml)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["## Version 3 - streaming select"],"metadata":{}},{"cell_type":"code","source":["DRIVER_FILE = \"mmlspark_streamselect.py\"\n\ndriver_file = '''\nimport os\nimport json\nfrom time import sleep\nfrom uuid import uuid4\nfrom zipfile import ZipFile\n\nfrom azureml.core.model import Model\nfrom pyspark.ml import PipelineModel\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType\nimport requests\n\n\ndef init():\n    \"\"\"One time initialization of pyspark and model server\"\"\"\n\n    spark = SparkSession.builder.appName(\"Model Server\").getOrCreate()\n    import mmlspark  # this is needed to load mmlspark libraries\n\n    # extract and load model\n    model_path = Model.get_model_path('{model_name}')\n    with ZipFile(model_path, 'r') as f:\n        f.extractall('model')\n    model = PipelineModel.load('model')\n\n    # load data schema saved with model\n    with open(os.path.join('model', 'schema.json'), 'r') as f:\n        schema = StructType.fromJson(json.load(f))\n\n    input_df = (\n        spark.readStream.continuousServer()\n        .address(\"localhost\", 8089, \"predict\")\n        .load()\n        .parseRequest(schema)\n    )\n\n    output_df = (\n        input_df.select(\"cat00\")\n        .makeReply(\"probability\")\n    )\n\n    checkpoint = os.path.join('/tmp', 'checkpoints', uuid4().hex)\n    server = (\n        output_df.writeStream.continuousServer()\n        .trigger(continuous=\"1 second\")\n        .replyTo(\"predict\")\n        .queryName(\"prediction\")\n        .option(\"checkpointLocation\", checkpoint)\n        .start()\n    )\n\n    # let the server finish starting\n    sleep(1)\n\n\ndef run(input_json):\n    try:\n        response = requests.post(data=input_json, url='http://localhost:8089/predict')\n        result = response.json()['cat00']\n    except Exception as e:\n        result = str(e)\n    \n    return json.dumps({{\"result\": result}})\n    \n'''.format(model_name=MODEL_NAME)\n\n# check syntax\nexec(driver_file)\n\nwith open(DRIVER_FILE, \"w\") as f:\n    f.write(driver_file)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["## Version 4 - base select"],"metadata":{}},{"cell_type":"code","source":["DRIVER_FILE = \"mmlspark_baseselect.py\"\n\nscore_sparkml = \"\"\"\n\nimport json\nfrom zipfile import ZipFile\n \ndef init():\n    # One-time initialization of PySpark and predictive model\n    import pyspark\n    from pyspark.ml import PipelineModel\n    from mmlspark import LightGBMClassifier\n    from azureml.core.model import Model\n    from pyspark.ml import PipelineModel\n    from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n    global trainedModel\n    global spark\n    global schema\n\n    spark = pyspark.sql.SparkSession.builder.appName(\"LightGBM Criteo Predictions\").getOrCreate()\n    model_path = Model.get_model_path('{model_name}')\n    with ZipFile(model_path, 'r') as f:\n        f.extractall('model')\n    trainedModel = PipelineModel.load('model')\n    \ndef run(input_json):\n    if isinstance(trainedModel, Exception):\n        return json.dumps({{\"trainedModel\":str(trainedModel)}})\n      \n    try:\n        sc = spark.sparkContext\n        input_list = json.loads(input_json)\n        input_rdd = sc.parallelize(input_list)\n        input_df = spark.read.json(input_rdd)\n        \n        # Compute prediction - in this case, just a select\n        predictions = input_df.select('cat00').collect()\n        #Get probability of a click for each row and conver to a str\n        click_prob = [x.cat00 for x in predictions]\n\n        # you can return any data type as long as it is JSON-serializable\n        result = \",\".join(click_prob)\n        return [result]\n    except Exception as e:\n        result = str(e)\n        return result\n\"\"\".format(model_name=MODEL_NAME)\n \nexec(score_sparkml)\n \nwith open(DRIVER_FILE, \"w\") as file:\n    file.write(score_sparkml)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["%sh\n\nls mmlspark*py"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">mmlspark_basescore.py\nmmlspark_baseselect.py\nmmlspark_serving.py\nmmlspark_streamingscore.py\nmmlspark_streamscore.py\nmmlspark_streamselect.py\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":18}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.8","nbconvert_exporter":"python","file_extension":".py"},"name":"deploy-to-aci-04","notebookId":2469253441220050,"kernelspec":{"display_name":"Python (reco_base)","language":"python","name":"reco_base"},"authors":[{"name":"pasha"}]},"nbformat":4,"nbformat_minor":0}
