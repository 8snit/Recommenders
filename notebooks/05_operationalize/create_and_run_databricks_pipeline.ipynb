{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a Notebook on Databricks\n",
    "\n",
    "This notebook runs a notebook on azure databricks by creating a `DatabricksCompute` and a `DatabricksStep` within an Azure Machine Learning Pipeline.\n",
    "\n",
    "This is based on the example notebook [here](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.runconfig import JarLibrary\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core.runconfig import PyPiLibrary, JarLibrary, EggLibrary\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Databricks notebook path\n",
    "notebook_path=os.getenv(\"DATABRICKS_NOTEBOOK_PATH\", \"/Users/jeremr@microsoft.com/parallel_top10/rescore_top10\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://eastus.azuredatabricks.net/files/top10/aml_config/config.json?o=4604276322347170\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace with your account info before running.\n",
    " \n",
    "db_compute_name=os.getenv(\"DATABRICKS_COMPUTE_NAME\", \"\") # Databricks compute name\n",
    "db_resource_group=os.getenv(\"DATABRICKS_RESOURCE_GROUP\", \"\") # Databricks resource group\n",
    "db_workspace_name=os.getenv(\"DATABRICKS_WORKSPACE_NAME\", \"\") # Databricks workspace name\n",
    "db_access_token=os.getenv(\"DATABRICKS_ACCESS_TOKEN\", \"\") # Databricks access token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
    "    print('Compute target {} already exists'.format(db_compute_name))\n",
    "except ComputeTargetException:\n",
    "    print('Compute not found, will use below parameters to attach new one')\n",
    "    print('db_compute_name {}'.format(db_compute_name))\n",
    "    print('db_resource_group {}'.format(db_resource_group))\n",
    "    print('db_workspace_name {}'.format(db_workspace_name))\n",
    "    print('db_access_token {}'.format(db_access_token))\n",
    "    config = DatabricksCompute.attach_configuration(\n",
    "        resource_group = db_resource_group,\n",
    "        workspace_name = db_workspace_name,\n",
    "        access_token= db_access_token)\n",
    "    databricks_compute=ComputeTarget.attach(ws, db_compute_name, config)\n",
    "    databricks_compute.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Scoring Script\n",
    "\n",
    "Assumptions for running this script:\n",
    "\n",
    "- `notebook_path` exists in the databricks workspace.\n",
    "- The dependencies of the notebook living at `notebook_path` exist.\n",
    "- The specified libraries exist on `dbfs`. These should honor the defaults in the databricks setup scripts in [../../scripts](../../scripts)\n",
    "- The default notebook does the operationalization parts of the [als_movie_o16n.ipynb](als_movie_o16n.ipynb) file. It de-serializes the estimated ALS model, creates recommendations, then writes to CosmosDB. Therefore, the default script requires:\n",
    "  - There is already a model estimated and serialized. The default is that there is a  `dbfs:/FileStore/top10/models/mvl-als-reco.mml`. \n",
    "  - There is a `secrets.json` file available on the cluster that controls access to the CosmosDB cluster that it scores to.\n",
    "\n",
    "The easiest way to fullfil these requirements is to run through the first half of the [als_movie_o16n.ipynb](als_movie_o16n.ipynb) notebook (Through Section 2), and then make sure that the files where you have serialized the `.mml` file and `secrets.json` match the expectations in the default script.\n",
    "\n",
    "The example below creates a new cluster for each run, but this can add a substantial amount of time. To run against an already existing cluster, you can pass the `DatabricksStep()` function a parameter `existing_cluster_id`.\n",
    "\n",
    "**TODO**: Update defaults of `als_movie_o16n.ipynb` to write to a persistent location that can serve as the default of the rescore notebook. Update the rescore notebook to honor those defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This works, but still requires interactive authentication.\n",
    "\n",
    "dbNbStep = DatabricksStep(\n",
    "    name=\"DBNotebookInWS\",\n",
    "    spark_version=\"4.3.x-scala2.11\",\n",
    "    num_workers=8,\n",
    "    notebook_path=notebook_path,\n",
    "    run_name='rescore_top10',\n",
    "    pypi_libraries=[PyPiLibrary(package=\"azureml-sdk[databricks]\", repo=None)],\n",
    "    egg_libraries=[EggLibrary(library=\"dbfs:/FileStore/jars/Recommenders.egg\")],\n",
    "    jar_libraries=[JarLibrary(library='dbfs:/FileStore/jars/azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar')],\n",
    "    compute_target=databricks_compute,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [dbNbStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'rescore_top10').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python [conda env:recommenders]",
   "language": "python",
   "name": "conda-env-recommenders-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
