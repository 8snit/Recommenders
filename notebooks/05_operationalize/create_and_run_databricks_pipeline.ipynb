{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a Notebook on Databricks\n",
    "\n",
    "This notebook runs a notebook on azure databricks by creating a `DatabricksCompute` and a `DatabricksStep` within an Azure Machine Learning Pipeline.\n",
    "\n",
    "This is based on the example notebook [here](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Scoring Script\n",
    "\n",
    "Assumptions for running the pipeline created below:\n",
    "\n",
    "- You have created an Azure Machine Learning Workspace and have used the `write_config()` to save its information in the current directory.\n",
    "- `notebook_path` exists in the databricks workspace. By default, that notebook is [rescore_top10.ipynb](rescore_top10.ipynb). If you haven't already, you should upload that notebook to your databricks workspace, or choose a notebook of your liking that is already in your workspace.\n",
    "- The dependencies of the notebook living at `notebook_path` exist.\n",
    "- Any specified libraries exist on `dbfs`. These should honor the defaults in the databricks setup scripts in [../../scripts](../../scripts)\n",
    "- The default notebook on databricks ([rescore_top10.ipynb](rescore_top10.ipynb)) does the operationalization parts of the [als_movie_o16n.ipynb](als_movie_o16n.ipynb) notebook. Specifically, it de-serializes the estimated ALS model, creates recommendations, then writes the top 10 recommendations for each person to CosmosDB. Therefore, the default script requires:\n",
    "  - There is already a model estimated and serialized. The default is that there is a  `dbfs:/FileStore/top10/models/mvl-als-reco.mml`. \n",
    "  - There is a `secrets.json` file available on the cluster that controls access to the CosmosDB cluster that it scores to.\n",
    "  - Relevant libraries to be available on dbfs (see below). Strictly speaking, the `Recommenders.egg` file is not necessarily, but this shows how you can specify an `EggLibrary`.\n",
    "\n",
    "The easiest way to fullfil these requirements is to run through the first half of the [als_movie_o16n.ipynb](als_movie_o16n.ipynb) notebook (Through Section 2), and then make sure that the files where you have serialized the `.mml` file and `secrets.json` match the expectations in the default script.\n",
    "\n",
    "The example below creates a new cluster for each run, but this can add a substantial amount of time. To run against an already existing cluster, you can pass the `DatabricksStep()` function a parameter `existing_cluster_id`.\n",
    "\n",
    "**TODO**: Update defaults of `als_movie_o16n.ipynb` to write to a persistent location that can serve as the default of the rescore notebook. Update the rescore notebook to honor those defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.runconfig import JarLibrary\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core.runconfig import PyPiLibrary, JarLibrary, EggLibrary\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Databricks notebook path\n",
    "notebook_path=os.getenv(\"DATABRICKS_NOTEBOOK_PATH\", \"/Users/jeremr@microsoft.com/parallel_top10/rescore_top10\")\n",
    "# if path_to_aml_config, it will look for ./aml_config/config.json\n",
    "# if it is not Nonte, you must pass the full path to config.json\n",
    "path_to_aml_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assumes you have an Azure Machine Learning workspace already\n",
    "## and that you saved the config in the current directory\n",
    "try: \n",
    "    ws = Workspace.from_config(path_to_aml_config)\n",
    "except:\n",
    "    message = \"\"\"\n",
    "        \n",
    "        Cannot find and load previously created workspace.\n",
    "        \n",
    "        path_to_aml_config={0}\n",
    "        \n",
    "        Either the path is wrong, or you must create a workspace and write the configuration. \n",
    "        You should create a workspace with:\n",
    "\n",
    "        ws=Workspace.create(name=<NAME>, \n",
    "            subscription_id=<SUB_ID>, \n",
    "            resource_group=<RESOURCE_GROUP>, \n",
    "            location=<LOCATION>)\n",
    "        ws.write_config()\n",
    "        \n",
    "\"\"\"\n",
    "    print(message.format(path_to_aml_config))\n",
    "    \n",
    "## make this raise the Error to stop:\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace with your account info before running.\n",
    " \n",
    "db_compute_name=os.getenv(\"DATABRICKS_COMPUTE_NAME\", \"\") # Databricks compute name\n",
    "db_resource_group=os.getenv(\"DATABRICKS_RESOURCE_GROUP\", \"\") # Databricks resource group\n",
    "db_workspace_name=os.getenv(\"DATABRICKS_WORKSPACE_NAME\", \"\") # Databricks workspace name\n",
    "db_access_token=os.getenv(\"DATABRICKS_ACCESS_TOKEN\", \"\") # Databricks access token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create the compute resource\n",
    "try:\n",
    "    databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
    "    print('Compute target {} already exists'.format(db_compute_name))\n",
    "except ComputeTargetException:\n",
    "    print('Compute not found, will use below parameters to attach new one')\n",
    "    print('db_compute_name {}'.format(db_compute_name))\n",
    "    print('db_resource_group {}'.format(db_resource_group))\n",
    "    print('db_workspace_name {}'.format(db_workspace_name))\n",
    "    print('db_access_token {}'.format(db_access_token))\n",
    "    config = DatabricksCompute.attach_configuration(\n",
    "        resource_group = db_resource_group,\n",
    "        workspace_name = db_workspace_name,\n",
    "        access_token= db_access_token)\n",
    "    databricks_compute=ComputeTarget.attach(ws, db_compute_name, config)\n",
    "    databricks_compute.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create the step\n",
    "dbNbStep = DatabricksStep(\n",
    "    name=\"DBNotebookInWS\",\n",
    "    spark_version=\"4.3.x-scala2.11\",\n",
    "    num_workers=8,\n",
    "    notebook_path=notebook_path,\n",
    "    run_name='rescore_top10',\n",
    "    pypi_libraries=[PyPiLibrary(package=\"azureml-sdk[databricks]\", repo=None)],\n",
    "    egg_libraries=[EggLibrary(library=\"dbfs:/FileStore/jars/Recommenders.egg\")],\n",
    "    jar_libraries=[JarLibrary(library='dbfs:/FileStore/jars/azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar')],\n",
    "    compute_target=databricks_compute,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create and execute the pipeline\n",
    "steps = [dbNbStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'rescore_top10').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python [conda env:recommenders]",
   "language": "python",
   "name": "conda-env-recommenders-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
