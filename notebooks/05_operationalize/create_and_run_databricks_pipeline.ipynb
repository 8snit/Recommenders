{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a notebook on databricks via runs submit\n",
    "\n",
    "This notebook runs a notebook on azure databricks\n",
    "\n",
    "Potential issue: it will need to use a service principal to authenticate to AML to log into the AML workspace to track the model\n",
    "\n",
    "# So, do this via a pipeline...\n",
    "\n",
    "based on the example notebook [here](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-use-databricks-as-compute-target.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.runconfig import JarLibrary\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://eastus.azuredatabricks.net/files/top10/aml_config/config.json?o=4604276322347170\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace with your account info before running.\n",
    " \n",
    "db_compute_name=os.getenv(\"DATABRICKS_COMPUTE_NAME\", \"\") # Databricks compute name\n",
    "db_resource_group=os.getenv(\"DATABRICKS_RESOURCE_GROUP\", \"\") # Databricks resource group\n",
    "db_workspace_name=os.getenv(\"DATABRICKS_WORKSPACE_NAME\", \"\") # Databricks workspace name\n",
    "db_access_token=os.getenv(\"DATABRICKS_ACCESS_TOKEN\", \"\") # Databricks access token\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
    "    print('Compute target {} already exists'.format(db_compute_name))\n",
    "except ComputeTargetException:\n",
    "    print('Compute not found, will use below parameters to attach new one')\n",
    "    print('db_compute_name {}'.format(db_compute_name))\n",
    "    print('db_resource_group {}'.format(db_resource_group))\n",
    "    print('db_workspace_name {}'.format(db_workspace_name))\n",
    "    print('db_access_token {}'.format(db_access_token))\n",
    "    config = DatabricksCompute.attach_configuration(\n",
    "        resource_group = db_resource_group,\n",
    "        workspace_name = db_workspace_name,\n",
    "        access_token= db_access_token)\n",
    "    databricks_compute=ComputeTarget.attach(ws, db_compute_name, config)\n",
    "    databricks_compute.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the default blob storage\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print('Datastore {} will be used'.format(def_blob_store.name))\n",
    "\n",
    "# We are uploading a sample file in the local directory to be used as a datasource\n",
    "def_blob_store.upload_files(files=[\"./testdata.txt\"], target_path=\"dbtest\", overwrite=False)\n",
    "\n",
    "step_1_input = DataReference(datastore=def_blob_store, path_on_datastore=\"dbtest\",\n",
    "                                     data_reference_name=\"input\")\n",
    "\n",
    "step_1_output = PipelineData(\"output\", datastore=def_blob_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls aml_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "runconfig = RunConfiguration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runconfig.load(path='.', name='top10_adb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?runconfig.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default blob storage\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print('Datastore {} will be used'.format(def_blob_store.name))\n",
    "\n",
    "# We are uploading a sample file in the local directory to be used as a datasource\n",
    "def_blob_store.upload_files(files=[\"./testdata.txt\"], target_path=\"dbtest\", overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_1_input = DataReference(datastore=def_blob_store, path_on_datastore=\"dbtest\",\n",
    "                                     data_reference_name=\"input\")\n",
    "\n",
    "step_1_output = PipelineData(\"output\", datastore=def_blob_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notebook_path=os.getenv(\"DATABRICKS_NOTEBOOK_PATH\", \"/Users/jeremr@microsoft.com/sample-notebook-for-pipeline\") # Databricks notebook path\n",
    "\n",
    "dbNbStep = DatabricksStep(\n",
    "    name=\"DBNotebookInWS\",\n",
    "    inputs=[step_1_input],\n",
    "    outputs=[step_1_output],\n",
    "    spark_version=\"4.3.x-scala2.11\",\n",
    "    num_workers=1,\n",
    "    notebook_path=notebook_path,\n",
    "    notebook_params={'myparam': 'testparam'},\n",
    "    run_name='DB_Notebook_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    allow_reuse=False\n",
    ")\n",
    "## need to add runconfig to add libraries\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [dbNbStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_Notebook_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notebook_path=os.getenv(\"DATABRICKS_NOTEBOOK_PATH\", \"/Users/jeremr@microsoft.com/parallel_top10/rescore_top10\") # Databricks notebook path\n",
    "from azureml.core.runconfig import PyPiLibrary, JarLibrary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This works, but still requires interactive authentication.\n",
    "\n",
    "dbNbStep = DatabricksStep(\n",
    "    name=\"DBNotebookInWS\",\n",
    "#    inputs=[step_1_input],\n",
    "#    outputs=[step_1_output],\n",
    "#    notebook_params={'myparam': 'testparam'},\n",
    "    spark_version=\"4.3.x-scala2.11\",\n",
    "    num_workers=8,\n",
    "    notebook_path=notebook_path,\n",
    "    run_name='rescore_top10',\n",
    "    pypi_libraries=[PyPiLibrary(package=\"azureml-sdk[databricks]\", repo=None)],\n",
    "    jar_libraries=[JarLibrary(library='dbfs:/FileStore/jars/azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar')],\n",
    "    compute_target=databricks_compute,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [dbNbStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'rescore_top10').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:recommenders]",
   "language": "python",
   "name": "conda-env-recommenders-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
