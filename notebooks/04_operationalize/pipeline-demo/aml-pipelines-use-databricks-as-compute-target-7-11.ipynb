{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Databricks as a Compute Target from Azure Machine Learning Pipeline\n",
    "To use Databricks as a compute target from [Azure Machine Learning Pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines), a [DatabricksStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.databricks_step.databricksstep?view=azure-ml-py) is used. This notebook demonstrates the use of DatabricksStep in Azure Machine Learning Pipeline.\n",
    "\n",
    "The notebook will show:\n",
    "1. Running an arbitrary Databricks notebook that the customer has in Databricks workspace\n",
    "2. Running an arbitrary Python script that the customer has in DBFS\n",
    "3. Running an arbitrary Python script that is available on local computer (will upload to DBFS, and then run in Databricks) \n",
    "4. Running a JAR job that the customer has in DBFS.\n",
    "\n",
    "## Before you begin:\n",
    "\n",
    "1. **Create an Azure Databricks workspace** in the same subscription where you have your Azure Machine Learning workspace. You will need details of this workspace later on to define DatabricksStep. [Click here](https://ms.portal.azure.com/#blade/HubsExtension/Resources/resourceType/Microsoft.Databricks%2Fworkspaces) for more information.\n",
    "2. **Create PAT (access token)**: Manually create a Databricks access token at the Azure Databricks portal. See [this](https://docs.databricks.com/api/latest/authentication.html#generate-a-token) for more information.\n",
    "3. **Add demo notebook to ADB**: This notebook has a sample you can use as is. Launch Azure Databricks attached to your Azure Machine Learning workspace and add a new notebook. \n",
    "4. **Create/attach a Blob storage** for use from ADB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"03909a66-bef8-4d52-8e9a-a346604e0902\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"AMLtestye\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"testamlye711\")\n",
    "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"southcentralus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access your workspace\n",
    "The following cell uses the Azure ML SDK to attempt to load the workspace specified by your parameters. If this cell succeeds, your notebook library will be configured to access the workspace from all notebooks using the Workspace.from_config() method. The cell can fail if the specified workspace doesn't exist or you don't have permissions to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote the config file config.json to: /data/home/adminye/notebooks/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml_config/config.json\n",
      "Workspace configuration succeeded. Skip the workspace creation steps below\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    # write the details of the workspace to a configuration file to the notebook library\n",
    "    ws.write_config()\n",
    "    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\n",
    "except:\n",
    "    print(\"Workspace not accessible. Change your parameters or create a new workspace below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core.runconfig import JarLibrary\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. Make sure the config file is present at .\\config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /data/home/adminye/notebooks/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml_config/config.json\n",
      "testamlye711\n",
      "AMLtestye\n",
      "southcentralus\n",
      "03909a66-bef8-4d52-8e9a-a346604e0902\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Databricks compute target\n",
    "Next, you need to add your Databricks workspace to Azure Machine Learning as a compute target and give it a name. You will use this name to refer to your Databricks workspace compute target inside Azure Machine Learning.\n",
    "\n",
    "- **Resource Group** - The resource group name of your Azure Machine Learning workspace\n",
    "- **Databricks Workspace Name** - The workspace name of your Azure Databricks workspace\n",
    "- **Databricks Access Token** - The access token you created in ADB\n",
    "\n",
    "**The Databricks workspace need to be present in the same subscription as your AML workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute target testye already exists\n"
     ]
    }
   ],
   "source": [
    "# Replace with your account info before running.\n",
    " \n",
    "db_compute_name=os.getenv(\"DATABRICKS_COMPUTE_NAME\", \"testye\") # Databricks compute name 'testye' aml compute\n",
    "db_resource_group=os.getenv(\"DATABRICKS_RESOURCE_GROUP\", \"AMLtestye2\") # Databricks resource group\n",
    "db_workspace_name=os.getenv(\"DATABRICKS_WORKSPACE_NAME\", \"testAMLye\") # Databricks workspace name\n",
    "db_access_token=os.getenv(\"DATABRICKS\",\"dapic7ffd9afe482076bbd884cc745027123\") # Databricks access token\n",
    " \n",
    "try:\n",
    "    databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
    "    print('Compute target {} already exists'.format(db_compute_name))\n",
    "except ComputeTargetException:\n",
    "    print('Compute not found, will use below parameters to attach new one')\n",
    "    print('db_compute_name {}'.format(db_compute_name))\n",
    "    print('db_resource_group {}'.format(db_resource_group))\n",
    "    print('db_workspace_name {}'.format(db_workspace_name))\n",
    "    print('db_access_token {}'.format(db_access_token))\n",
    " \n",
    "    config = DatabricksCompute.attach_configuration(\n",
    "        resource_group = db_resource_group,\n",
    "        workspace_name = db_workspace_name,\n",
    "        access_token= db_access_token)\n",
    "    databricks_compute=ComputeTarget.attach(ws, db_compute_name, config)\n",
    "    databricks_compute.wait_for_completion(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Databricks from Azure Machine Learning Pipeline\n",
    "To use Databricks as a compute target from Azure Machine Learning Pipeline, a DatabricksStep is used. Let's define a datasource (via DataReference) and intermediate data (via PipelineData) to be used in DatabricksStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found blob datastore with name: MyBlobDatastore\n",
      "Datastore myblobdatastore will be used\n"
     ]
    }
   ],
   "source": [
    "from msrest.exceptions import HttpOperationError\n",
    "\n",
    "blob_datastore_name='MyBlobDatastore'\n",
    "\n",
    "account_name=os.getenv(\"BLOB_ACCOUNTNAME_62\", \"amltestyediag\") # Storage account name\n",
    "container_name=os.getenv(\"BLOB_CONTAINER_62\", \"movielens\") # Name of Azure blob container\n",
    "account_key=os.getenv(\"BLOB_ACCOUNT_KEY_62\", \"6YSqy31inkACB1o8lV+mXS+ph+Na7LBMW3HidOZ3wUkHCFJBtMeVW6hkvzgxhKv9waezK4qPfsw4TFPILx1oVw==\") # Storage account key\n",
    "\n",
    "##connect data in blob to Datastore\n",
    "try:\n",
    "    blob_datastore = Datastore.get(ws, blob_datastore_name)\n",
    "    print(\"found blob datastore with name: %s\" % blob_datastore_name)\n",
    "except HttpOperationError:\n",
    "    blob_datastore = Datastore.register_azure_blob_container(\n",
    "        workspace=ws,\n",
    "        datastore_name=blob_datastore_name,\n",
    "        account_name=account_name, # Storage account name\n",
    "        container_name=container_name, # Name of Azure blob container\n",
    "        account_key=account_key) # Storage account key\"\n",
    "    print(\"registered blob datastore with name: %s\" % blob_datastore_name)\n",
    "print('Datastore {} will be used'.format(blob_datastore.name))\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default blob storage\n",
    "#def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "#print('Datastore {} will be used'.format(def_blob_store.name))\n",
    "\n",
    "# We are uploading a sample file in the local directory to be used as a datasource\n",
    "#def_blob_store.upload_files(files=[\"./testdata.txt\"], target_path=\"dbtest\", overwrite=False)\n",
    "\n",
    "step_1_input = DataReference(datastore=blob_datastore, path_on_datastore=\"movielens\",\n",
    "                                     data_reference_name=\"input\")\n",
    "\n",
    "step_1_output = PipelineData(\"output\", datastore=blob_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_datastore = Datastore.get(ws, blob_datastore_name)\n",
    "print(\"found blob datastore with name: %s\" % blob_datastore_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Running the demo notebook already added to the Databricks workspace\n",
    "Create a notebook in the Azure Databricks workspace, and provide the path to that notebook as the value associated with the environment variable \"DATABRICKS_NOTEBOOK_PATH\". This will then set the variableÂ notebook_pathÂ when you run the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.core.runconfig.RunConfiguration at 0x7f62d9f64c50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "runconfig = RunConfiguration()\n",
    "runconfig.load(path='.', name='library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path=os.getenv(\"DATABRICKS_NOTEBOOK_PATH\", \"/Users/yexing@microsoft.com/als_deep_dive\") # Databricks notebook path\n",
    "\n",
    "dbNbStep = DatabricksStep(\n",
    "    name=\"DBNotebookInWS\", #name of the step\n",
    "    inputs=[step_1_input],\n",
    "    outputs=[step_1_output],\n",
    "    num_workers=1,\n",
    "    notebook_path=notebook_path,\n",
    "    notebook_params={'myparam': 'testparam'},  ## what is this one?\n",
    "    run_name='DB_Notebook_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    runconfig=runconfig,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and submit the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step DBNotebookInWS [5aa645cf][38e22330-0146-4d24-9cb6-9a95011c762f], (This step will run and generate new outputs)\n",
      "Using data reference input for StepId [c91f1265][15efa834-d51e-49d6-8fe5-c53b5fc81110], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted pipeline run: 5b2515b8-878f-4890-b73e-aac9647997e6\n",
      "status:Running\n",
      "..................................................................................................\n",
      "status:Failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Failed'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [dbNbStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_Notebook_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running a Python script that is already added in DBFS\n",
    "To run a Python script that is already uploaded to DBFS, follow the instructions below. You will first upload the Python script to DBFS using the [CLI](https://docs.azuredatabricks.net/user-guide/dbfs-databricks-file-system.html).\n",
    "\n",
    "The commented out code in the below cell assumes that you have uploaded `als_deep_dive.py` to the root folder in DBFS. You can upload `als_deep_dive.py` to the folder \"scripts\" in DBFS using this commandline so you can use `python_script_path = \"dbfs:/scripts/train-db-dbfs.py\"`:\n",
    "\n",
    "```\n",
    "dbfs mkdirs dbfs:/scripts\n",
    "dbfs cp ./als_deep_dive.py dbfs:/scripts/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_path = \"dbfs:/scripts/als_deep_dive.py\"\n",
    "\n",
    "dbPythonInDbfsStep = DatabricksStep(\n",
    "    name=\"DBPythonInDBFS\",\n",
    "    inputs=[step_1_input],\n",
    "    num_workers=1,\n",
    "    python_script_path=python_script_path,\n",
    "    python_script_params={'--input_data'},\n",
    "    run_name='DB_Python_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and submit the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [dbPythonInDbfsStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_Python_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running a Python script in Databricks that currenlty is in local computer\n",
    "To run a Python script that is currently in your local computer, follow the instructions below. \n",
    "\n",
    "The commented out code below code assumes that you have `als_deep_dive.py` in the `scripts` subdirectory under the current working directory.\n",
    "\n",
    "In this case, the Python script will be uploaded first to DBFS, and then the script will be run in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_name = \"als_deep_dive.py\"\n",
    "source_directory = \".\"  ##does it need to be under \"scripts\" folder?\n",
    "\n",
    "dbPythonInLocalMachineStep = DatabricksStep(\n",
    "    name=\"DBPythonInLocalMachine\",\n",
    "    inputs=[step_1_input],\n",
    "    num_workers=1,\n",
    "    python_script_name=python_script_name,\n",
    "    source_directory=source_directory,\n",
    "    run_name='DB_Python_Local_demo',\n",
    "    compute_target=databricks_compute,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and submit the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [dbPythonInLocalMachineStep]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DB_Python_Local_demo').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "diray"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
