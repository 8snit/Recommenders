{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xDeepFM : the eXtreme Deep Factorization Machine \n",
    "This notebook will give you a quick example of how to train an xDeepFM model. \n",
    "xDeepFM \\[1\\] is a deep learning-based model aims at capturing both lower- and higher-order feature interactions for precise recommender systems. Thus it can learn feature interactions more effectively and manual feature engineering effort can be substantially reduced. To summarize, xDeepFM has the following key properties:\n",
    "* It contains a component, named CIN, that learns feature interactions in an explicit fashion and in vector-wise level;\n",
    "* It contains a traditional DNN component that learns feature interactions in an implicit fashion and in bit-wise level.\n",
    "* The implementation makes this model quite configurable. We can enable different subsets of components by setting hyperparameters like `use_Linear_part`, `use_FM_part`, `use_CIN_part`, and `use_DNN_part`. For example, by enabling only the `use_Linear_part` and `use_FM_part`, we can get a classical FM model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.5.5 |Anaconda custom (64-bit)| (default, May 13 2018, 21:12:35) \n",
      "[GCC 7.2.0]\n",
      "Tensorflow version: 1.10.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import papermill as pm\n",
    "import tensorflow as tf\n",
    "\n",
    "from reco_utils.recommender.deeprec.deeprec_utils import *\n",
    "from reco_utils.recommender.deeprec.models.xDeepFM import *\n",
    "from reco_utils.recommender.deeprec.IO.iterator import *\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#EPOCHS_FOR_SYNTHETIC_RUN = 15\n",
    "#EPOCHS_FOR_CRITEO_RUN = 30\n",
    "#BATCH_SIZE_SYNTHETIC = 128\n",
    "#BATCH_SIZE_CRITEO = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "xDeepFM uses the FFM format as data input: `<label> <field_id>:<feature_id>:<feature_value>`  \n",
    "Each line represents an instance, `<label>` is a binary value with 1 meaning positive instance and 0 meaning negative instance. \n",
    "Features are divided into fields. For example, user's gender is a field, it contains three possible values, i.e. male, female and unknown. Occupation can be another field, which contains many more possible values than the gender field. Both field index and feature index are starting from 1. <br>\n",
    "Now let's start with movielens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing movielens dataset. Please go to ../../tests/resources/deeprec/movielens folder. \n",
    "#ML-100K2Libffm.py loads user rating data with movie gener data\n",
    "# ML-100K2Libffm.py transforms into libffm format <field_id>:<feature_id>:<feature_value>. \n",
    "\n",
    "#wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "#unzip ml-100k.zip\n",
    "#python ML-100K2Libffm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../tests/resources/deeprec/movielens'\n",
    "yaml_file = os.path.join(data_path, r'network_xdeepFM.yaml')\n",
    "#train_file = os.path.join(data_path, r'ua.base.classification.final')\n",
    "#train_file = os.path.join(data_path, r'ua.base.regression.final')\n",
    "#valid_file = os.path.join(data_path, r'ua.test.regression.final')\n",
    "#test_file = os.path.join(data_path, r'ua.test.regression.final')\n",
    "\n",
    "####the following files are for classification\n",
    "train_file = os.path.join(data_path, r'ua.base.classification.final')\n",
    "valid_file = os.path.join(data_path, r'ua.test.classification.final')\n",
    "test_file = os.path.join(data_path, r'ua.test.classification.final')\n",
    "\n",
    "#test_file = os.path.join(data_path, r'ua.test.classification_topN.final')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "#if not os.path.exists(yaml_file):\n",
    "#    download_deeprec_resources(r'https://recodatasets.blob.core.windows.net/deeprec/', data_path, 'xdeepfmresources.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameters\n",
    "prepare_hparams() will create a full set of hyper-parameters for model training, such as learning rate, feature number, and dropout ratio. We can put those parameters in a yaml file, or pass parameters as the function's parameters (which will overwrite yaml settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DNN_FIELD_NUM', None), ('FEATURE_COUNT', 213), ('FIELD_COUNT', 5), ('MODEL_DIR', None), ('PAIR_NUM', None), ('SUMMARIES_DIR', None), ('activation', ['relu', 'relu']), ('attention_activation', None), ('attention_dropout', 0.0), ('attention_layer_sizes', None), ('batch_size', 128), ('cross_activation', 'identity'), ('cross_l1', 0.0), ('cross_l2', 0.0), ('cross_layer_sizes', [100, 100, 50]), ('cross_layers', None), ('data_format', 'ffm'), ('dim', 10), ('doc_size', None), ('dropout', [0.0, 0.0]), ('dtype', 32), ('embed_l1', 0.0), ('embed_l2', 0.0), ('enable_BN', False), ('entityEmb_file', None), ('entity_dim', None), ('entity_embedding_method', None), ('entity_size', None), ('epochs', 10), ('fast_CIN_d', 0), ('filter_sizes', None), ('init_method', 'tnormal'), ('init_value', 0.01), ('is_clip_norm', 0), ('iterator_type', None), ('kg_file', None), ('kg_training_interval', 5), ('layer_l1', 0.0), ('layer_l2', 0.0), ('layer_sizes', [400, 400]), ('learning_rate', 0.001), ('load_model_name', None), ('load_saved_model', False), ('loss', 'log_loss'), ('lr_kg', 0.5), ('lr_rs', 1), ('max_grad_norm', 2), ('method', 'classification'), ('metrics', ['rmse', 'mae', 'rsquare', 'exp_var', 'auc', 'logloss']), ('model_type', 'xDeepFM'), ('mu', None), ('n_item', None), ('n_item_attr', None), ('n_user', None), ('n_user_attr', None), ('num_filters', None), ('optimizer', 'adam'), ('reg_kg', 0.0), ('save_epoch', 2), ('save_model', False), ('show_step', 20), ('train_ratio', None), ('transform', None), ('use_CIN_part', True), ('use_DNN_part', False), ('use_FM_part', False), ('use_Linear_part', False), ('user_clicks', None), ('user_dropout', False), ('wordEmb_file', None), ('word_size', None), ('write_tfevents', False)]\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file) ##\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader\n",
    "Designate a data iterator for the model. xDeepFM uses FFMTextIterator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = FFMTextIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "When both hyper-parameters and data iterator are ready, we can create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add CIN part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model = XDeepFMModel(hparams, input_creator)\n",
    "\n",
    "## sometimes we don't want to train a model from scratch\n",
    "## then we can load a pre-trained model like this: \n",
    "#model.load_model(r'your_model_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what is the model's performance at this point (without starting training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.run_eval(test_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC=0.5 is a state of random guess. We can see that before training, the model behaves like random guessing. Next we want to train the model on a training set, and check the performance on a validation dataset. Training the model is as simple as a function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 0.6897, data_loss: 0.6897\n",
      "step 40 , total_loss: 0.6849, data_loss: 0.6849\n",
      "step 60 , total_loss: 0.6796, data_loss: 0.6796\n",
      "step 80 , total_loss: 0.7847, data_loss: 0.7847\n",
      "step 100 , total_loss: 0.6977, data_loss: 0.6977\n",
      "step 120 , total_loss: 0.6618, data_loss: 0.6618\n",
      "step 140 , total_loss: 0.7070, data_loss: 0.7070\n",
      "step 160 , total_loss: 0.6447, data_loss: 0.6447\n",
      "step 180 , total_loss: 0.6987, data_loss: 0.6987\n",
      "step 200 , total_loss: 0.7039, data_loss: 0.7039\n",
      "step 220 , total_loss: 0.6430, data_loss: 0.6430\n",
      "step 240 , total_loss: 0.6372, data_loss: 0.6372\n",
      "step 260 , total_loss: 0.6415, data_loss: 0.6415\n",
      "step 280 , total_loss: 0.6041, data_loss: 0.6041\n",
      "step 300 , total_loss: 0.6991, data_loss: 0.6991\n",
      "step 320 , total_loss: 0.8631, data_loss: 0.8631\n",
      "step 340 , total_loss: 0.6868, data_loss: 0.6868\n",
      "step 360 , total_loss: 0.6192, data_loss: 0.6192\n",
      "step 380 , total_loss: 0.6641, data_loss: 0.6641\n",
      "step 400 , total_loss: 0.6668, data_loss: 0.6668\n",
      "step 420 , total_loss: 0.7078, data_loss: 0.7078\n",
      "step 440 , total_loss: 0.7151, data_loss: 0.7151\n",
      "step 460 , total_loss: 0.6165, data_loss: 0.6165\n",
      "step 480 , total_loss: 0.7031, data_loss: 0.7031\n",
      "step 500 , total_loss: 0.6312, data_loss: 0.6312\n",
      "step 520 , total_loss: 0.6245, data_loss: 0.6245\n",
      "step 540 , total_loss: 0.6743, data_loss: 0.6743\n",
      "step 560 , total_loss: 0.6739, data_loss: 0.6739\n",
      "step 580 , total_loss: 0.6679, data_loss: 0.6679\n",
      "step 600 , total_loss: 0.6465, data_loss: 0.6465\n",
      "step 620 , total_loss: 0.6966, data_loss: 0.6966\n",
      "step 640 , total_loss: 0.6620, data_loss: 0.6620\n",
      "step 660 , total_loss: 0.6757, data_loss: 0.6757\n",
      "step 680 , total_loss: 0.6245, data_loss: 0.6245\n",
      "step 700 , total_loss: 0.5246, data_loss: 0.5246\n",
      "at epoch 1 train info: auc:0.6579, exp_var:0.07459110021591187, logloss:0.6491, mae:0.46251073, rmse:0.47853944, rsquare:0.07455982299333497 eval info: auc:0.6316, exp_var:0.05135905742645264, logloss:0.656, mae:0.46527138, rmse:0.48166376, rsquare:0.047695431591006754\n",
      "at epoch 1 , train time: 8.6 eval time: 6.3\n",
      "step 20 , total_loss: 0.5787, data_loss: 0.5787\n",
      "step 40 , total_loss: 0.6195, data_loss: 0.6195\n",
      "step 60 , total_loss: 0.6527, data_loss: 0.6527\n",
      "step 80 , total_loss: 0.8354, data_loss: 0.8354\n",
      "step 100 , total_loss: 0.7549, data_loss: 0.7549\n",
      "step 120 , total_loss: 0.6395, data_loss: 0.6395\n",
      "step 140 , total_loss: 0.6930, data_loss: 0.6930\n",
      "step 160 , total_loss: 0.6178, data_loss: 0.6178\n",
      "step 180 , total_loss: 0.6858, data_loss: 0.6858\n",
      "step 200 , total_loss: 0.7354, data_loss: 0.7354\n",
      "step 220 , total_loss: 0.6521, data_loss: 0.6521\n",
      "step 240 , total_loss: 0.5957, data_loss: 0.5957\n",
      "step 260 , total_loss: 0.6112, data_loss: 0.6112\n",
      "step 280 , total_loss: 0.5828, data_loss: 0.5828\n",
      "step 300 , total_loss: 0.6626, data_loss: 0.6626\n",
      "step 320 , total_loss: 0.8266, data_loss: 0.8266\n",
      "step 340 , total_loss: 0.7144, data_loss: 0.7144\n",
      "step 360 , total_loss: 0.5958, data_loss: 0.5958\n",
      "step 380 , total_loss: 0.7193, data_loss: 0.7193\n",
      "step 400 , total_loss: 0.6655, data_loss: 0.6655\n",
      "step 420 , total_loss: 0.7337, data_loss: 0.7337\n",
      "step 440 , total_loss: 0.7146, data_loss: 0.7146\n",
      "step 460 , total_loss: 0.6105, data_loss: 0.6105\n",
      "step 480 , total_loss: 0.6984, data_loss: 0.6984\n",
      "step 500 , total_loss: 0.6382, data_loss: 0.6382\n",
      "step 520 , total_loss: 0.6088, data_loss: 0.6088\n",
      "step 540 , total_loss: 0.6688, data_loss: 0.6688\n",
      "step 560 , total_loss: 0.6715, data_loss: 0.6715\n",
      "step 580 , total_loss: 0.6624, data_loss: 0.6624\n",
      "step 600 , total_loss: 0.6294, data_loss: 0.6294\n",
      "step 620 , total_loss: 0.7155, data_loss: 0.7155\n",
      "step 640 , total_loss: 0.6573, data_loss: 0.6573\n",
      "step 660 , total_loss: 0.6693, data_loss: 0.6693\n",
      "step 680 , total_loss: 0.6176, data_loss: 0.6176\n",
      "step 700 , total_loss: 0.5019, data_loss: 0.5019\n",
      "at epoch 2 train info: auc:0.6737, exp_var:0.09050405025482178, logloss:0.6406, mae:0.45364153, rmse:0.47434163, rsquare:0.09049806599857113 eval info: auc:0.6418, exp_var:0.05858445167541504, logloss:0.6521, mae:0.45877638, rmse:0.47958314, rsquare:0.05571744500472664\n",
      "at epoch 2 , train time: 7.8 eval time: 6.2\n",
      "step 20 , total_loss: 0.5729, data_loss: 0.5729\n",
      "step 40 , total_loss: 0.6074, data_loss: 0.6074\n",
      "step 60 , total_loss: 0.6511, data_loss: 0.6511\n",
      "step 80 , total_loss: 0.8831, data_loss: 0.8831\n",
      "step 100 , total_loss: 0.7498, data_loss: 0.7498\n",
      "step 120 , total_loss: 0.6328, data_loss: 0.6328\n",
      "step 140 , total_loss: 0.6949, data_loss: 0.6949\n",
      "step 160 , total_loss: 0.6076, data_loss: 0.6076\n",
      "step 180 , total_loss: 0.6803, data_loss: 0.6803\n",
      "step 200 , total_loss: 0.7482, data_loss: 0.7482\n",
      "step 220 , total_loss: 0.6630, data_loss: 0.6630\n",
      "step 240 , total_loss: 0.5762, data_loss: 0.5762\n",
      "step 260 , total_loss: 0.6070, data_loss: 0.6070\n",
      "step 280 , total_loss: 0.5829, data_loss: 0.5829\n",
      "step 300 , total_loss: 0.6511, data_loss: 0.6511\n",
      "step 320 , total_loss: 0.7779, data_loss: 0.7779\n",
      "step 340 , total_loss: 0.7118, data_loss: 0.7118\n",
      "step 360 , total_loss: 0.5754, data_loss: 0.5754\n",
      "step 380 , total_loss: 0.7448, data_loss: 0.7448\n",
      "step 400 , total_loss: 0.6658, data_loss: 0.6658\n",
      "step 420 , total_loss: 0.7342, data_loss: 0.7342\n",
      "step 440 , total_loss: 0.7179, data_loss: 0.7179\n",
      "step 460 , total_loss: 0.6043, data_loss: 0.6043\n",
      "step 480 , total_loss: 0.6827, data_loss: 0.6827\n",
      "step 500 , total_loss: 0.6425, data_loss: 0.6425\n",
      "step 520 , total_loss: 0.5975, data_loss: 0.5975\n",
      "step 540 , total_loss: 0.6763, data_loss: 0.6763\n",
      "step 560 , total_loss: 0.6652, data_loss: 0.6652\n",
      "step 580 , total_loss: 0.6623, data_loss: 0.6623\n",
      "step 600 , total_loss: 0.6245, data_loss: 0.6245\n",
      "step 620 , total_loss: 0.7158, data_loss: 0.7158\n",
      "step 640 , total_loss: 0.6588, data_loss: 0.6588\n",
      "step 660 , total_loss: 0.6630, data_loss: 0.6630\n",
      "step 680 , total_loss: 0.6237, data_loss: 0.6237\n",
      "step 700 , total_loss: 0.5040, data_loss: 0.5040\n",
      "at epoch 3 train info: auc:0.6851, exp_var:0.1025012731552124, logloss:0.6343, mae:0.4475683, rmse:0.47116876, rsquare:0.10245989835334512 eval info: auc:0.6488, exp_var:0.06346505880355835, logloss:0.6496, mae:0.45467657, rmse:0.4782259, rsquare:0.06099573966290628\n",
      "at epoch 3 , train time: 7.8 eval time: 6.2\n",
      "step 20 , total_loss: 0.5609, data_loss: 0.5609\n",
      "step 40 , total_loss: 0.6007, data_loss: 0.6007\n",
      "step 60 , total_loss: 0.6437, data_loss: 0.6437\n",
      "step 80 , total_loss: 0.8037, data_loss: 0.8037\n",
      "step 100 , total_loss: 0.7422, data_loss: 0.7422\n",
      "step 120 , total_loss: 0.6233, data_loss: 0.6233\n",
      "step 140 , total_loss: 0.6892, data_loss: 0.6892\n",
      "step 160 , total_loss: 0.5935, data_loss: 0.5935\n",
      "step 180 , total_loss: 0.6500, data_loss: 0.6500\n",
      "step 200 , total_loss: 0.7540, data_loss: 0.7540\n",
      "step 220 , total_loss: 0.6809, data_loss: 0.6809\n",
      "step 240 , total_loss: 0.5716, data_loss: 0.5716\n",
      "step 260 , total_loss: 0.6240, data_loss: 0.6240\n",
      "step 280 , total_loss: 0.5865, data_loss: 0.5865\n",
      "step 300 , total_loss: 0.6438, data_loss: 0.6438\n",
      "step 320 , total_loss: 0.7316, data_loss: 0.7316\n",
      "step 340 , total_loss: 0.7100, data_loss: 0.7100\n",
      "step 360 , total_loss: 0.5624, data_loss: 0.5624\n",
      "step 380 , total_loss: 0.7211, data_loss: 0.7211\n",
      "step 400 , total_loss: 0.6664, data_loss: 0.6664\n",
      "step 420 , total_loss: 0.7333, data_loss: 0.7333\n",
      "step 440 , total_loss: 0.7324, data_loss: 0.7324\n",
      "step 460 , total_loss: 0.5972, data_loss: 0.5972\n",
      "step 480 , total_loss: 0.6624, data_loss: 0.6624\n",
      "step 500 , total_loss: 0.6420, data_loss: 0.6420\n",
      "step 520 , total_loss: 0.5919, data_loss: 0.5919\n",
      "step 540 , total_loss: 0.6773, data_loss: 0.6773\n",
      "step 560 , total_loss: 0.6651, data_loss: 0.6651\n",
      "step 580 , total_loss: 0.6598, data_loss: 0.6598\n",
      "step 600 , total_loss: 0.6226, data_loss: 0.6226\n",
      "step 620 , total_loss: 0.7070, data_loss: 0.7070\n",
      "step 640 , total_loss: 0.6608, data_loss: 0.6608\n",
      "step 660 , total_loss: 0.6875, data_loss: 0.6875\n",
      "step 680 , total_loss: 0.6257, data_loss: 0.6257\n",
      "step 700 , total_loss: 0.5078, data_loss: 0.5078\n",
      "at epoch 4 train info: auc:0.6933, exp_var:0.11165416240692139, logloss:0.6294, mae:0.44427216, rmse:0.46882832, rsquare:0.11165401579564305 eval info: auc:0.6527, exp_var:0.06709146499633789, logloss:0.648, mae:0.45295885, rmse:0.47749346, rsquare:0.06414291704503872\n",
      "at epoch 4 , train time: 7.8 eval time: 6.2\n",
      "step 20 , total_loss: 0.5566, data_loss: 0.5566\n",
      "step 40 , total_loss: 0.5885, data_loss: 0.5885\n",
      "step 60 , total_loss: 0.6408, data_loss: 0.6408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 80 , total_loss: 0.7178, data_loss: 0.7178\n",
      "step 100 , total_loss: 0.7420, data_loss: 0.7420\n",
      "step 120 , total_loss: 0.6171, data_loss: 0.6171\n",
      "step 140 , total_loss: 0.6798, data_loss: 0.6798\n",
      "step 160 , total_loss: 0.5726, data_loss: 0.5726\n",
      "step 180 , total_loss: 0.6281, data_loss: 0.6281\n",
      "step 200 , total_loss: 0.7530, data_loss: 0.7530\n",
      "step 220 , total_loss: 0.6576, data_loss: 0.6576\n",
      "step 240 , total_loss: 0.5658, data_loss: 0.5658\n",
      "step 260 , total_loss: 0.6383, data_loss: 0.6383\n",
      "step 280 , total_loss: 0.5884, data_loss: 0.5884\n",
      "step 300 , total_loss: 0.6432, data_loss: 0.6432\n",
      "step 320 , total_loss: 0.6640, data_loss: 0.6640\n",
      "step 340 , total_loss: 0.7083, data_loss: 0.7083\n",
      "step 360 , total_loss: 0.5434, data_loss: 0.5434\n",
      "step 380 , total_loss: 0.7015, data_loss: 0.7015\n",
      "step 400 , total_loss: 0.6670, data_loss: 0.6670\n",
      "step 420 , total_loss: 0.7314, data_loss: 0.7314\n",
      "step 440 , total_loss: 0.7399, data_loss: 0.7399\n",
      "step 460 , total_loss: 0.5929, data_loss: 0.5929\n",
      "step 480 , total_loss: 0.6501, data_loss: 0.6501\n",
      "step 500 , total_loss: 0.6427, data_loss: 0.6427\n",
      "step 520 , total_loss: 0.5820, data_loss: 0.5820\n",
      "step 540 , total_loss: 0.6804, data_loss: 0.6804\n",
      "step 560 , total_loss: 0.6654, data_loss: 0.6654\n",
      "step 580 , total_loss: 0.6559, data_loss: 0.6559\n",
      "step 600 , total_loss: 0.6273, data_loss: 0.6273\n",
      "step 620 , total_loss: 0.7012, data_loss: 0.7012\n",
      "step 640 , total_loss: 0.6617, data_loss: 0.6617\n",
      "step 660 , total_loss: 0.6944, data_loss: 0.6944\n",
      "step 680 , total_loss: 0.6248, data_loss: 0.6248\n",
      "step 700 , total_loss: 0.5055, data_loss: 0.5055\n",
      "at epoch 5 train info: auc:0.6989, exp_var:0.11865192651748657, logloss:0.6255, mae:0.44112173, rmse:0.4669047, rsquare:0.11864457252869198 eval info: auc:0.6551, exp_var:0.0693557858467102, logloss:0.6468, mae:0.45110705, rmse:0.47686476, rsquare:0.06642236521620715\n",
      "at epoch 5 , train time: 7.7 eval time: 6.2\n",
      "step 20 , total_loss: 0.5503, data_loss: 0.5503\n",
      "step 40 , total_loss: 0.5807, data_loss: 0.5807\n",
      "step 60 , total_loss: 0.6440, data_loss: 0.6440\n",
      "step 80 , total_loss: 0.6655, data_loss: 0.6655\n",
      "step 100 , total_loss: 0.7390, data_loss: 0.7390\n",
      "step 120 , total_loss: 0.6110, data_loss: 0.6110\n",
      "step 140 , total_loss: 0.6717, data_loss: 0.6717\n",
      "step 160 , total_loss: 0.5536, data_loss: 0.5536\n",
      "step 180 , total_loss: 0.6178, data_loss: 0.6178\n",
      "step 200 , total_loss: 0.7505, data_loss: 0.7505\n",
      "step 220 , total_loss: 0.6367, data_loss: 0.6367\n",
      "step 240 , total_loss: 0.5596, data_loss: 0.5596\n",
      "step 260 , total_loss: 0.6416, data_loss: 0.6416\n",
      "step 280 , total_loss: 0.5902, data_loss: 0.5902\n",
      "step 300 , total_loss: 0.6433, data_loss: 0.6433\n",
      "step 320 , total_loss: 0.5961, data_loss: 0.5961\n",
      "step 340 , total_loss: 0.7071, data_loss: 0.7071\n",
      "step 360 , total_loss: 0.5320, data_loss: 0.5320\n",
      "step 380 , total_loss: 0.6825, data_loss: 0.6825\n",
      "step 400 , total_loss: 0.6661, data_loss: 0.6661\n",
      "step 420 , total_loss: 0.7246, data_loss: 0.7246\n",
      "step 440 , total_loss: 0.7399, data_loss: 0.7399\n",
      "step 460 , total_loss: 0.5913, data_loss: 0.5913\n",
      "step 480 , total_loss: 0.6405, data_loss: 0.6405\n",
      "step 500 , total_loss: 0.6404, data_loss: 0.6404\n",
      "step 520 , total_loss: 0.5717, data_loss: 0.5717\n",
      "step 540 , total_loss: 0.6896, data_loss: 0.6896\n",
      "step 560 , total_loss: 0.6666, data_loss: 0.6666\n",
      "step 580 , total_loss: 0.6532, data_loss: 0.6532\n",
      "step 600 , total_loss: 0.6372, data_loss: 0.6372\n",
      "step 620 , total_loss: 0.7012, data_loss: 0.7012\n",
      "step 640 , total_loss: 0.6613, data_loss: 0.6613\n",
      "step 660 , total_loss: 0.6921, data_loss: 0.6921\n",
      "step 680 , total_loss: 0.6255, data_loss: 0.6255\n",
      "step 700 , total_loss: 0.5021, data_loss: 0.5021\n",
      "at epoch 6 train info: auc:0.7025, exp_var:0.12375038862228394, logloss:0.6227, mae:0.43822336, rmse:0.46561787, rsquare:0.12374222281075886 eval info: auc:0.657, exp_var:0.0711895227432251, logloss:0.6457, mae:0.44917676, rmse:0.4763402, rsquare:0.06853911522110523\n",
      "at epoch 6 , train time: 7.9 eval time: 6.2\n",
      "step 20 , total_loss: 0.5426, data_loss: 0.5426\n",
      "step 40 , total_loss: 0.5757, data_loss: 0.5757\n",
      "step 60 , total_loss: 0.6483, data_loss: 0.6483\n",
      "step 80 , total_loss: 0.6274, data_loss: 0.6274\n",
      "step 100 , total_loss: 0.7343, data_loss: 0.7343\n",
      "step 120 , total_loss: 0.6062, data_loss: 0.6062\n",
      "step 140 , total_loss: 0.6656, data_loss: 0.6656\n",
      "step 160 , total_loss: 0.5437, data_loss: 0.5437\n",
      "step 180 , total_loss: 0.6154, data_loss: 0.6154\n",
      "step 200 , total_loss: 0.7466, data_loss: 0.7466\n",
      "step 220 , total_loss: 0.6231, data_loss: 0.6231\n",
      "step 240 , total_loss: 0.5590, data_loss: 0.5590\n",
      "step 260 , total_loss: 0.6394, data_loss: 0.6394\n",
      "step 280 , total_loss: 0.5911, data_loss: 0.5911\n",
      "step 300 , total_loss: 0.6432, data_loss: 0.6432\n",
      "step 320 , total_loss: 0.5370, data_loss: 0.5370\n",
      "step 340 , total_loss: 0.7038, data_loss: 0.7038\n",
      "step 360 , total_loss: 0.5243, data_loss: 0.5243\n",
      "step 380 , total_loss: 0.6670, data_loss: 0.6670\n",
      "step 400 , total_loss: 0.6647, data_loss: 0.6647\n",
      "step 420 , total_loss: 0.7189, data_loss: 0.7189\n",
      "step 440 , total_loss: 0.7394, data_loss: 0.7394\n",
      "step 460 , total_loss: 0.5912, data_loss: 0.5912\n",
      "step 480 , total_loss: 0.6328, data_loss: 0.6328\n",
      "step 500 , total_loss: 0.6362, data_loss: 0.6362\n",
      "step 520 , total_loss: 0.5620, data_loss: 0.5620\n",
      "step 540 , total_loss: 0.6980, data_loss: 0.6980\n",
      "step 560 , total_loss: 0.6691, data_loss: 0.6691\n",
      "step 580 , total_loss: 0.6504, data_loss: 0.6504\n",
      "step 600 , total_loss: 0.6482, data_loss: 0.6482\n",
      "step 620 , total_loss: 0.7031, data_loss: 0.7031\n",
      "step 640 , total_loss: 0.6601, data_loss: 0.6601\n",
      "step 660 , total_loss: 0.6875, data_loss: 0.6875\n",
      "step 680 , total_loss: 0.6277, data_loss: 0.6277\n",
      "step 700 , total_loss: 0.4970, data_loss: 0.4970\n",
      "at epoch 7 train info: auc:0.7046, exp_var:0.1270245909690857, logloss:0.6208, mae:0.43564305, rmse:0.464758, rsquare:0.12702265589531825 eval info: auc:0.6582, exp_var:0.07229173183441162, logloss:0.645, mae:0.44733667, rmse:0.47602522, rsquare:0.06997095937580133\n",
      "at epoch 7 , train time: 7.9 eval time: 6.2\n",
      "step 20 , total_loss: 0.5343, data_loss: 0.5343\n",
      "step 40 , total_loss: 0.5735, data_loss: 0.5735\n",
      "step 60 , total_loss: 0.6513, data_loss: 0.6513\n",
      "step 80 , total_loss: 0.5901, data_loss: 0.5901\n",
      "step 100 , total_loss: 0.7283, data_loss: 0.7283\n",
      "step 120 , total_loss: 0.6034, data_loss: 0.6034\n",
      "step 140 , total_loss: 0.6652, data_loss: 0.6652\n",
      "step 160 , total_loss: 0.5401, data_loss: 0.5401\n",
      "step 180 , total_loss: 0.6178, data_loss: 0.6178\n",
      "step 200 , total_loss: 0.7430, data_loss: 0.7430\n",
      "step 220 , total_loss: 0.6142, data_loss: 0.6142\n",
      "step 240 , total_loss: 0.5607, data_loss: 0.5607\n",
      "step 260 , total_loss: 0.6356, data_loss: 0.6356\n",
      "step 280 , total_loss: 0.5914, data_loss: 0.5914\n",
      "step 300 , total_loss: 0.6430, data_loss: 0.6430\n",
      "step 320 , total_loss: 0.4864, data_loss: 0.4864\n",
      "step 340 , total_loss: 0.6992, data_loss: 0.6992\n",
      "step 360 , total_loss: 0.5203, data_loss: 0.5203\n",
      "step 380 , total_loss: 0.6584, data_loss: 0.6584\n",
      "step 400 , total_loss: 0.6639, data_loss: 0.6639\n",
      "step 420 , total_loss: 0.7150, data_loss: 0.7150\n",
      "step 440 , total_loss: 0.7385, data_loss: 0.7385\n",
      "step 460 , total_loss: 0.5920, data_loss: 0.5920\n",
      "step 480 , total_loss: 0.6278, data_loss: 0.6278\n",
      "step 500 , total_loss: 0.6335, data_loss: 0.6335\n",
      "step 520 , total_loss: 0.5544, data_loss: 0.5544\n",
      "step 540 , total_loss: 0.7027, data_loss: 0.7027\n",
      "step 560 , total_loss: 0.6714, data_loss: 0.6714\n",
      "step 580 , total_loss: 0.6485, data_loss: 0.6485\n",
      "step 600 , total_loss: 0.6564, data_loss: 0.6564\n",
      "step 620 , total_loss: 0.7048, data_loss: 0.7048\n",
      "step 640 , total_loss: 0.6596, data_loss: 0.6596\n",
      "step 660 , total_loss: 0.6823, data_loss: 0.6823\n",
      "step 680 , total_loss: 0.6297, data_loss: 0.6297\n",
      "step 700 , total_loss: 0.4927, data_loss: 0.4927\n",
      "at epoch 8 train info: auc:0.7058, exp_var:0.12909597158432007, logloss:0.6196, mae:0.43339276, rmse:0.46421978, rsquare:0.1290954518063674 eval info: auc:0.6589, exp_var:0.07282936573028564, logloss:0.6446, mae:0.44573653, rmse:0.4758151, rsquare:0.07069818728431443\n",
      "at epoch 8 , train time: 7.8 eval time: 6.3\n",
      "step 20 , total_loss: 0.5279, data_loss: 0.5279\n",
      "step 40 , total_loss: 0.5730, data_loss: 0.5730\n",
      "step 60 , total_loss: 0.6509, data_loss: 0.6509\n",
      "step 80 , total_loss: 0.5535, data_loss: 0.5535\n",
      "step 100 , total_loss: 0.7227, data_loss: 0.7227\n",
      "step 120 , total_loss: 0.6019, data_loss: 0.6019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 140 , total_loss: 0.6682, data_loss: 0.6682\n",
      "step 160 , total_loss: 0.5386, data_loss: 0.5386\n",
      "step 180 , total_loss: 0.6219, data_loss: 0.6219\n",
      "step 200 , total_loss: 0.7412, data_loss: 0.7412\n",
      "step 220 , total_loss: 0.6084, data_loss: 0.6084\n",
      "step 240 , total_loss: 0.5623, data_loss: 0.5623\n",
      "step 260 , total_loss: 0.6326, data_loss: 0.6326\n",
      "step 280 , total_loss: 0.5914, data_loss: 0.5914\n",
      "step 300 , total_loss: 0.6427, data_loss: 0.6427\n",
      "step 320 , total_loss: 0.4490, data_loss: 0.4490\n",
      "step 340 , total_loss: 0.6956, data_loss: 0.6956\n",
      "step 360 , total_loss: 0.5195, data_loss: 0.5195\n",
      "step 380 , total_loss: 0.6544, data_loss: 0.6544\n",
      "step 400 , total_loss: 0.6635, data_loss: 0.6635\n",
      "step 420 , total_loss: 0.7136, data_loss: 0.7136\n",
      "step 440 , total_loss: 0.7365, data_loss: 0.7365\n",
      "step 460 , total_loss: 0.5926, data_loss: 0.5926\n",
      "step 480 , total_loss: 0.6251, data_loss: 0.6251\n",
      "step 500 , total_loss: 0.6335, data_loss: 0.6335\n",
      "step 520 , total_loss: 0.5495, data_loss: 0.5495\n",
      "step 540 , total_loss: 0.7067, data_loss: 0.7067\n",
      "step 560 , total_loss: 0.6729, data_loss: 0.6729\n",
      "step 580 , total_loss: 0.6482, data_loss: 0.6482\n",
      "step 600 , total_loss: 0.6605, data_loss: 0.6605\n",
      "step 620 , total_loss: 0.7053, data_loss: 0.7053\n",
      "step 640 , total_loss: 0.6597, data_loss: 0.6597\n",
      "step 660 , total_loss: 0.6779, data_loss: 0.6779\n",
      "step 680 , total_loss: 0.6308, data_loss: 0.6308\n",
      "step 700 , total_loss: 0.4896, data_loss: 0.4896\n",
      "at epoch 9 train info: auc:0.7068, exp_var:0.13062626123428345, logloss:0.6187, mae:0.43152693, rmse:0.46378875, rsquare:0.13062570367956794 eval info: auc:0.6596, exp_var:0.07323604822158813, logloss:0.6444, mae:0.4444223, rmse:0.47571, rsquare:0.0711800271086237\n",
      "at epoch 9 , train time: 7.8 eval time: 6.2\n",
      "step 20 , total_loss: 0.5234, data_loss: 0.5234\n",
      "step 40 , total_loss: 0.5732, data_loss: 0.5732\n",
      "step 60 , total_loss: 0.6478, data_loss: 0.6478\n",
      "step 80 , total_loss: 0.5214, data_loss: 0.5214\n",
      "step 100 , total_loss: 0.7174, data_loss: 0.7174\n",
      "step 120 , total_loss: 0.6011, data_loss: 0.6011\n",
      "step 140 , total_loss: 0.6714, data_loss: 0.6714\n",
      "step 160 , total_loss: 0.5386, data_loss: 0.5386\n",
      "step 180 , total_loss: 0.6264, data_loss: 0.6264\n",
      "step 200 , total_loss: 0.7409, data_loss: 0.7409\n",
      "step 220 , total_loss: 0.6048, data_loss: 0.6048\n",
      "step 240 , total_loss: 0.5629, data_loss: 0.5629\n",
      "step 260 , total_loss: 0.6304, data_loss: 0.6304\n",
      "step 280 , total_loss: 0.5913, data_loss: 0.5913\n",
      "step 300 , total_loss: 0.6422, data_loss: 0.6422\n",
      "step 320 , total_loss: 0.4270, data_loss: 0.4270\n",
      "step 340 , total_loss: 0.6935, data_loss: 0.6935\n",
      "step 360 , total_loss: 0.5204, data_loss: 0.5204\n",
      "step 380 , total_loss: 0.6522, data_loss: 0.6522\n",
      "step 400 , total_loss: 0.6631, data_loss: 0.6631\n",
      "step 420 , total_loss: 0.7134, data_loss: 0.7134\n",
      "step 440 , total_loss: 0.7343, data_loss: 0.7343\n",
      "step 460 , total_loss: 0.5926, data_loss: 0.5926\n",
      "step 480 , total_loss: 0.6240, data_loss: 0.6240\n",
      "step 500 , total_loss: 0.6351, data_loss: 0.6351\n",
      "step 520 , total_loss: 0.5469, data_loss: 0.5469\n",
      "step 540 , total_loss: 0.7113, data_loss: 0.7113\n",
      "step 560 , total_loss: 0.6737, data_loss: 0.6737\n",
      "step 580 , total_loss: 0.6487, data_loss: 0.6487\n",
      "step 600 , total_loss: 0.6614, data_loss: 0.6614\n",
      "step 620 , total_loss: 0.7038, data_loss: 0.7038\n",
      "step 640 , total_loss: 0.6599, data_loss: 0.6599\n",
      "step 660 , total_loss: 0.6746, data_loss: 0.6746\n",
      "step 680 , total_loss: 0.6321, data_loss: 0.6321\n",
      "step 700 , total_loss: 0.4881, data_loss: 0.4881\n",
      "at epoch 10 train info: auc:0.7077, exp_var:0.13194489479064941, logloss:0.618, mae:0.43009555, rmse:0.4634652, rsquare:0.13194293597064932 eval info: auc:0.6605, exp_var:0.07379478216171265, logloss:0.6442, mae:0.44340557, rmse:0.47549975, rsquare:0.07173032307047911\n",
      "at epoch 10 , train time: 7.8 eval time: 6.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<reco_utils.recommender.deeprec.models.xDeepFM.XDeepFMModel at 0x7fca841b0518>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_file, valid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's see what is the model's performance now (after training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mae': 0.44340557, 'rsquare': 0.07173032307047911, 'logloss': 0.6442, 'rmse': 0.47549975, 'exp_var': 0.07379478216171265, 'auc': 0.6605}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "res_syn": {
        "auc": 0.6605,
        "exp_var": 0.07379478216171265,
        "logloss": 0.6442,
        "mae": 0.4434055685997009,
        "rmse": 0.4754997491836548,
        "rsquare": 0.07173032307047911
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_syn = model.run_eval(test_file)\n",
    "print(res_syn)\n",
    "pm.record(\"res_syn\", res_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ranking metric: map_at_k = 0.008857395925597878\n",
    "ndcg_at_k = 1.0\n",
    "precision_at_k = 1.0\n",
    "recall_at_k = 0.008857395925597878\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\\[1\\] Lian, J., Zhou, X., Zhang, F., Chen, Z., Xie, X., & Sun, G. (2018). xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems.Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, KDD 2018, London, UK, August 19-23, 2018.<br>\n",
    "\\[2\\] The Criteo datasets: http://labs.criteo.com/category/dataset/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
