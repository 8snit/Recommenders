{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xDeepFM : the eXtreme Deep Factorization Machine \n",
    "This notebook will give you a quick example of how to train an xDeepFM model. \n",
    "xDeepFM \\[1\\] is a deep learning-based model aims at capturing both lower- and higher-order feature interactions for precise recommender systems. Thus it can learn feature interactions more effectively and manual feature engineering effort can be substantially reduced. To summarize, xDeepFM has the following key properties:\n",
    "* It contains a component, named CIN, that learns feature interactions in an explicit fashion and in vector-wise level;\n",
    "* It contains a traditional DNN component that learns feature interactions in an implicit fashion and in bit-wise level.\n",
    "* The implementation makes this model quite configurable. We can enable different subsets of components by setting hyperparameters like `use_Linear_part`, `use_FM_part`, `use_CIN_part`, and `use_DNN_part`. For example, by enabling only the `use_Linear_part` and `use_FM_part`, we can get a classical FM model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.5.5 |Anaconda custom (64-bit)| (default, May 13 2018, 21:12:35) \n",
      "[GCC 7.2.0]\n",
      "Tensorflow version: 1.10.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import papermill as pm\n",
    "import tensorflow as tf\n",
    "\n",
    "from reco_utils.recommender.deeprec.deeprec_utils import *\n",
    "from reco_utils.recommender.deeprec.models.xDeepFM import *\n",
    "from reco_utils.recommender.deeprec.IO.iterator import *\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#EPOCHS_FOR_SYNTHETIC_RUN = 15\n",
    "#EPOCHS_FOR_CRITEO_RUN = 30\n",
    "#BATCH_SIZE_SYNTHETIC = 128\n",
    "#BATCH_SIZE_CRITEO = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "xDeepFM uses the FFM format as data input: `<label> <field_id>:<feature_id>:<feature_value>`  \n",
    "Each line represents an instance, `<label>` is a binary value with 1 meaning positive instance and 0 meaning negative instance. \n",
    "Features are divided into fields. For example, user's gender is a field, it contains three possible values, i.e. male, female and unknown. Occupation can be another field, which contains many more possible values than the gender field. Both field index and feature index are starting from 1. <br>\n",
    "Now let's start with movielens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing movielens dataset. Please go to ../../tests/resources/deeprec/movielens folder. \n",
    "#ML-100K2Libffm.py loads user rating data with movie gener data\n",
    "# ML-100K2Libffm.py transforms into libffm format <field_id>:<feature_id>:<feature_value>. \n",
    "\n",
    "#wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "#unzip ml-100k.zip\n",
    "#python ML-100K2Libffm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../tests/resources/deeprec/movielens'\n",
    "yaml_file = os.path.join(data_path, r'network_xdeepFM.yaml')\n",
    "#train_file = os.path.join(data_path, r'ua.base.classification.final')\n",
    "#train_file = os.path.join(data_path, r'ua.base.regression.final')\n",
    "#valid_file = os.path.join(data_path, r'ua.test.regression.final')\n",
    "#test_file = os.path.join(data_path, r'ua.test.regression.final')\n",
    "\n",
    "####the following files are for classification\n",
    "train_file = os.path.join(data_path, r'ua.base.classification.final')\n",
    "valid_file = os.path.join(data_path, r'ua.test.classification.final')\n",
    "#test_file = os.path.join(data_path, r'ua.test.classification.final')\n",
    "test_file = os.path.join(data_path, r'ua.test.classification_topN.final')\n",
    "\n",
    "#test_file = os.path.join(data_path, r'ua.test.classification_topN.final')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "#if not os.path.exists(yaml_file):\n",
    "#    download_deeprec_resources(r'https://recodatasets.blob.core.windows.net/deeprec/', data_path, 'xdeepfmresources.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameters\n",
    "prepare_hparams() will create a full set of hyper-parameters for model training, such as learning rate, feature number, and dropout ratio. We can put those parameters in a yaml file, or pass parameters as the function's parameters (which will overwrite yaml settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DNN_FIELD_NUM', None), ('FEATURE_COUNT', 213), ('FIELD_COUNT', 5), ('MODEL_DIR', None), ('PAIR_NUM', None), ('SUMMARIES_DIR', None), ('activation', ['relu', 'relu']), ('attention_activation', None), ('attention_dropout', 0.0), ('attention_layer_sizes', None), ('batch_size', 128), ('cross_activation', 'identity'), ('cross_l1', 0.0), ('cross_l2', 0.0), ('cross_layer_sizes', [100, 100, 50]), ('cross_layers', None), ('data_format', 'ffm'), ('dim', 10), ('doc_size', None), ('dropout', [0.0, 0.0]), ('dtype', 32), ('embed_l1', 0.0), ('embed_l2', 0.0), ('enable_BN', False), ('entityEmb_file', None), ('entity_dim', None), ('entity_embedding_method', None), ('entity_size', None), ('epochs', 10), ('fast_CIN_d', 0), ('filter_sizes', None), ('init_method', 'tnormal'), ('init_value', 0.01), ('is_clip_norm', 0), ('iterator_type', None), ('kg_file', None), ('kg_training_interval', 5), ('layer_l1', 0.0), ('layer_l2', 0.0), ('layer_sizes', [400, 400]), ('learning_rate', 0.001), ('load_model_name', None), ('load_saved_model', False), ('loss', 'log_loss'), ('lr_kg', 0.5), ('lr_rs', 1), ('max_grad_norm', 2), ('method', 'classification'), ('metrics', ['rmse', 'mae', 'rsquare', 'exp_var', 'auc', 'logloss']), ('model_type', 'xDeepFM'), ('mu', None), ('n_item', None), ('n_item_attr', None), ('n_user', None), ('n_user_attr', None), ('num_filters', None), ('optimizer', 'adam'), ('ranking_metrics', ['map_at_k', 'ndcg_at_k', 'precision_at_k', 'recall_at_k']), ('reg_kg', 0.0), ('save_epoch', 2), ('save_model', False), ('show_step', 20), ('top_K', 10), ('train_ratio', None), ('transform', None), ('use_CIN_part', True), ('use_DNN_part', False), ('use_FM_part', False), ('use_Linear_part', False), ('user_clicks', None), ('user_dropout', False), ('user_item_file', '../../tests/resources/deeprec/movielens/user_item.csv'), ('wordEmb_file', None), ('word_size', None), ('write_tfevents', False)]\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file) ##\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader\n",
    "Designate a data iterator for the model. xDeepFM uses FFMTextIterator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = FFMTextIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "When both hyper-parameters and data iterator are ready, we can create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add CIN part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model = XDeepFMModel(hparams, input_creator)\n",
    "\n",
    "## sometimes we don't want to train a model from scratch\n",
    "## then we can load a pre-trained model like this: \n",
    "#model.load_model(r'your_model_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what is the model's performance at this point (without starting training):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and fit model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 0.6903, data_loss: 0.6903\n",
      "step 40 , total_loss: 0.6876, data_loss: 0.6876\n",
      "step 60 , total_loss: 0.6820, data_loss: 0.6820\n",
      "step 80 , total_loss: 0.7653, data_loss: 0.7653\n",
      "step 100 , total_loss: 0.6709, data_loss: 0.6709\n",
      "step 120 , total_loss: 0.6529, data_loss: 0.6529\n",
      "step 140 , total_loss: 0.6902, data_loss: 0.6902\n",
      "step 160 , total_loss: 0.6162, data_loss: 0.6162\n",
      "step 180 , total_loss: 0.6748, data_loss: 0.6748\n",
      "step 200 , total_loss: 0.7213, data_loss: 0.7213\n",
      "step 220 , total_loss: 0.6183, data_loss: 0.6183\n",
      "step 240 , total_loss: 0.6238, data_loss: 0.6238\n",
      "step 260 , total_loss: 0.6671, data_loss: 0.6671\n",
      "step 280 , total_loss: 0.5815, data_loss: 0.5815\n",
      "step 300 , total_loss: 0.6722, data_loss: 0.6722\n",
      "step 320 , total_loss: 0.7872, data_loss: 0.7872\n",
      "step 340 , total_loss: 0.6791, data_loss: 0.6791\n",
      "step 360 , total_loss: 0.6202, data_loss: 0.6202\n",
      "step 380 , total_loss: 0.7010, data_loss: 0.7010\n",
      "step 400 , total_loss: 0.6599, data_loss: 0.6599\n",
      "step 420 , total_loss: 0.6977, data_loss: 0.6977\n",
      "step 440 , total_loss: 0.7384, data_loss: 0.7384\n",
      "step 460 , total_loss: 0.6237, data_loss: 0.6237\n",
      "step 480 , total_loss: 0.6748, data_loss: 0.6748\n",
      "step 500 , total_loss: 0.6188, data_loss: 0.6188\n",
      "step 520 , total_loss: 0.6105, data_loss: 0.6105\n",
      "step 540 , total_loss: 0.7267, data_loss: 0.7267\n",
      "step 560 , total_loss: 0.6773, data_loss: 0.6773\n",
      "step 580 , total_loss: 0.6666, data_loss: 0.6666\n",
      "step 600 , total_loss: 0.6479, data_loss: 0.6479\n",
      "step 620 , total_loss: 0.6975, data_loss: 0.6975\n",
      "step 640 , total_loss: 0.6717, data_loss: 0.6717\n",
      "step 660 , total_loss: 0.6506, data_loss: 0.6506\n",
      "step 680 , total_loss: 0.6513, data_loss: 0.6513\n",
      "step 700 , total_loss: 0.5487, data_loss: 0.5487\n",
      "at epoch 1 train info: auc:0.6661, exp_var:0.08230161666870117, logloss:0.6451, mae:0.45689923, rmse:0.47644517, rsquare:0.08229093199448789 eval info: auc:0.6398, exp_var:0.05721163749694824, logloss:0.6528, mae:0.460651, rmse:0.48, rsquare:0.05405174898409859\n",
      "at epoch 1 , train time: 9.4 eval time: 6.0\n",
      "step 20 , total_loss: 0.5524, data_loss: 0.5524\n",
      "step 40 , total_loss: 0.6439, data_loss: 0.6439\n",
      "step 60 , total_loss: 0.6357, data_loss: 0.6357\n",
      "step 80 , total_loss: 0.8485, data_loss: 0.8485\n",
      "step 100 , total_loss: 0.7129, data_loss: 0.7129\n",
      "step 120 , total_loss: 0.6302, data_loss: 0.6302\n",
      "step 140 , total_loss: 0.6994, data_loss: 0.6994\n",
      "step 160 , total_loss: 0.6176, data_loss: 0.6176\n",
      "step 180 , total_loss: 0.7002, data_loss: 0.7002\n",
      "step 200 , total_loss: 0.7480, data_loss: 0.7480\n",
      "step 220 , total_loss: 0.5903, data_loss: 0.5903\n",
      "step 240 , total_loss: 0.5613, data_loss: 0.5613\n",
      "step 260 , total_loss: 0.6337, data_loss: 0.6337\n",
      "step 280 , total_loss: 0.5606, data_loss: 0.5606\n",
      "step 300 , total_loss: 0.6510, data_loss: 0.6510\n",
      "step 320 , total_loss: 0.7668, data_loss: 0.7668\n",
      "step 340 , total_loss: 0.6712, data_loss: 0.6712\n",
      "step 360 , total_loss: 0.5845, data_loss: 0.5845\n",
      "step 380 , total_loss: 0.7147, data_loss: 0.7147\n",
      "step 400 , total_loss: 0.6639, data_loss: 0.6639\n",
      "step 420 , total_loss: 0.7047, data_loss: 0.7047\n",
      "step 440 , total_loss: 0.7573, data_loss: 0.7573\n",
      "step 460 , total_loss: 0.6159, data_loss: 0.6159\n",
      "step 480 , total_loss: 0.6881, data_loss: 0.6881\n",
      "step 500 , total_loss: 0.5919, data_loss: 0.5919\n",
      "step 520 , total_loss: 0.5432, data_loss: 0.5432\n",
      "step 540 , total_loss: 0.7352, data_loss: 0.7352\n",
      "step 560 , total_loss: 0.6774, data_loss: 0.6774\n",
      "step 580 , total_loss: 0.6685, data_loss: 0.6685\n",
      "step 600 , total_loss: 0.6358, data_loss: 0.6358\n",
      "step 620 , total_loss: 0.7077, data_loss: 0.7077\n",
      "step 640 , total_loss: 0.6595, data_loss: 0.6595\n",
      "step 660 , total_loss: 0.6484, data_loss: 0.6484\n",
      "step 680 , total_loss: 0.6403, data_loss: 0.6403\n",
      "step 700 , total_loss: 0.5414, data_loss: 0.5414\n",
      "at epoch 2 train info: auc:0.6819, exp_var:0.09923321008682251, logloss:0.6361, mae:0.4483685, rmse:0.47201696, rsquare:0.09922642807517201 eval info: auc:0.652, exp_var:0.06711483001708984, logloss:0.6482, mae:0.4540576, rmse:0.47770283, rsquare:0.06331927840264084\n",
      "at epoch 2 , train time: 8.4 eval time: 6.1\n",
      "step 20 , total_loss: 0.5548, data_loss: 0.5548\n",
      "step 40 , total_loss: 0.6317, data_loss: 0.6317\n",
      "step 60 , total_loss: 0.6281, data_loss: 0.6281\n",
      "step 80 , total_loss: 0.8140, data_loss: 0.8140\n",
      "step 100 , total_loss: 0.6974, data_loss: 0.6974\n",
      "step 120 , total_loss: 0.6274, data_loss: 0.6274\n",
      "step 140 , total_loss: 0.7090, data_loss: 0.7090\n",
      "step 160 , total_loss: 0.6320, data_loss: 0.6320\n",
      "step 180 , total_loss: 0.7175, data_loss: 0.7175\n",
      "step 200 , total_loss: 0.7437, data_loss: 0.7437\n",
      "step 220 , total_loss: 0.5841, data_loss: 0.5841\n",
      "step 240 , total_loss: 0.5371, data_loss: 0.5371\n",
      "step 260 , total_loss: 0.6249, data_loss: 0.6249\n",
      "step 280 , total_loss: 0.5507, data_loss: 0.5507\n",
      "step 300 , total_loss: 0.6516, data_loss: 0.6516\n",
      "step 320 , total_loss: 0.7209, data_loss: 0.7209\n",
      "step 340 , total_loss: 0.6593, data_loss: 0.6593\n",
      "step 360 , total_loss: 0.5505, data_loss: 0.5505\n",
      "step 380 , total_loss: 0.6788, data_loss: 0.6788\n",
      "step 400 , total_loss: 0.6779, data_loss: 0.6779\n",
      "step 420 , total_loss: 0.7125, data_loss: 0.7125\n",
      "step 440 , total_loss: 0.7622, data_loss: 0.7622\n",
      "step 460 , total_loss: 0.6119, data_loss: 0.6119\n",
      "step 480 , total_loss: 0.6874, data_loss: 0.6874\n",
      "step 500 , total_loss: 0.5891, data_loss: 0.5891\n",
      "step 520 , total_loss: 0.5261, data_loss: 0.5261\n",
      "step 540 , total_loss: 0.7534, data_loss: 0.7534\n",
      "step 560 , total_loss: 0.6809, data_loss: 0.6809\n",
      "step 580 , total_loss: 0.6610, data_loss: 0.6610\n",
      "step 600 , total_loss: 0.6334, data_loss: 0.6334\n",
      "step 620 , total_loss: 0.7022, data_loss: 0.7022\n",
      "step 640 , total_loss: 0.6534, data_loss: 0.6534\n",
      "step 660 , total_loss: 0.6444, data_loss: 0.6444\n",
      "step 680 , total_loss: 0.6363, data_loss: 0.6363\n",
      "step 700 , total_loss: 0.5469, data_loss: 0.5469\n",
      "at epoch 3 train info: auc:0.6923, exp_var:0.11057925224304199, logloss:0.6301, mae:0.44420937, rmse:0.46914816, rsquare:0.11047470467184306 eval info: auc:0.6575, exp_var:0.07271212339401245, logloss:0.6456, mae:0.4515497, rmse:0.47644517, rsquare:0.06832197391992678\n",
      "at epoch 3 , train time: 8.3 eval time: 6.2\n",
      "step 20 , total_loss: 0.5542, data_loss: 0.5542\n",
      "step 40 , total_loss: 0.6294, data_loss: 0.6294\n",
      "step 60 , total_loss: 0.6310, data_loss: 0.6310\n",
      "step 80 , total_loss: 0.7495, data_loss: 0.7495\n",
      "step 100 , total_loss: 0.6937, data_loss: 0.6937\n",
      "step 120 , total_loss: 0.6282, data_loss: 0.6282\n",
      "step 140 , total_loss: 0.7155, data_loss: 0.7155\n",
      "step 160 , total_loss: 0.6436, data_loss: 0.6436\n",
      "step 180 , total_loss: 0.7092, data_loss: 0.7092\n",
      "step 200 , total_loss: 0.7404, data_loss: 0.7404\n",
      "step 220 , total_loss: 0.5855, data_loss: 0.5855\n",
      "step 240 , total_loss: 0.5340, data_loss: 0.5340\n",
      "step 260 , total_loss: 0.6106, data_loss: 0.6106\n",
      "step 280 , total_loss: 0.5498, data_loss: 0.5498\n",
      "step 300 , total_loss: 0.6554, data_loss: 0.6554\n",
      "step 320 , total_loss: 0.6588, data_loss: 0.6588\n",
      "step 340 , total_loss: 0.6582, data_loss: 0.6582\n",
      "step 360 , total_loss: 0.5279, data_loss: 0.5279\n",
      "step 380 , total_loss: 0.6667, data_loss: 0.6667\n",
      "step 400 , total_loss: 0.6759, data_loss: 0.6759\n",
      "step 420 , total_loss: 0.7096, data_loss: 0.7096\n",
      "step 440 , total_loss: 0.7549, data_loss: 0.7549\n",
      "step 460 , total_loss: 0.6091, data_loss: 0.6091\n",
      "step 480 , total_loss: 0.6773, data_loss: 0.6773\n",
      "step 500 , total_loss: 0.5941, data_loss: 0.5941\n",
      "step 520 , total_loss: 0.5240, data_loss: 0.5240\n",
      "step 540 , total_loss: 0.7613, data_loss: 0.7613\n",
      "step 560 , total_loss: 0.6815, data_loss: 0.6815\n",
      "step 580 , total_loss: 0.6574, data_loss: 0.6574\n",
      "step 600 , total_loss: 0.6458, data_loss: 0.6458\n",
      "step 620 , total_loss: 0.7015, data_loss: 0.7015\n",
      "step 640 , total_loss: 0.6511, data_loss: 0.6511\n",
      "step 660 , total_loss: 0.6413, data_loss: 0.6413\n",
      "step 680 , total_loss: 0.6376, data_loss: 0.6376\n",
      "step 700 , total_loss: 0.5441, data_loss: 0.5441\n",
      "at epoch 4 train info: auc:0.6985, exp_var:0.11824154853820801, logloss:0.6259, mae:0.4409492, rmse:0.46711883, rsquare:0.11809373183356386 eval info: auc:0.6609, exp_var:0.07624459266662598, logloss:0.6436, mae:0.4493974, rmse:0.47539458, rsquare:0.07211161793403609\n",
      "at epoch 4 , train time: 8.3 eval time: 6.3\n",
      "step 20 , total_loss: 0.5503, data_loss: 0.5503\n",
      "step 40 , total_loss: 0.6381, data_loss: 0.6381\n",
      "step 60 , total_loss: 0.6333, data_loss: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 80 , total_loss: 0.6821, data_loss: 0.6821\n",
      "step 100 , total_loss: 0.6905, data_loss: 0.6905\n",
      "step 120 , total_loss: 0.6283, data_loss: 0.6283\n",
      "step 140 , total_loss: 0.7133, data_loss: 0.7133\n",
      "step 160 , total_loss: 0.6490, data_loss: 0.6490\n",
      "step 180 , total_loss: 0.6986, data_loss: 0.6986\n",
      "step 200 , total_loss: 0.7408, data_loss: 0.7408\n",
      "step 220 , total_loss: 0.5873, data_loss: 0.5873\n",
      "step 240 , total_loss: 0.5347, data_loss: 0.5347\n",
      "step 260 , total_loss: 0.6016, data_loss: 0.6016\n",
      "step 280 , total_loss: 0.5482, data_loss: 0.5482\n",
      "step 300 , total_loss: 0.6572, data_loss: 0.6572\n",
      "step 320 , total_loss: 0.5996, data_loss: 0.5996\n",
      "step 340 , total_loss: 0.6610, data_loss: 0.6610\n",
      "step 360 , total_loss: 0.5230, data_loss: 0.5230\n",
      "step 380 , total_loss: 0.6623, data_loss: 0.6623\n",
      "step 400 , total_loss: 0.6724, data_loss: 0.6724\n",
      "step 420 , total_loss: 0.7089, data_loss: 0.7089\n",
      "step 440 , total_loss: 0.7482, data_loss: 0.7482\n",
      "step 460 , total_loss: 0.6120, data_loss: 0.6120\n",
      "step 480 , total_loss: 0.6682, data_loss: 0.6682\n",
      "step 500 , total_loss: 0.5927, data_loss: 0.5927\n",
      "step 520 , total_loss: 0.5225, data_loss: 0.5225\n",
      "step 540 , total_loss: 0.7741, data_loss: 0.7741\n",
      "step 560 , total_loss: 0.6799, data_loss: 0.6799\n",
      "step 580 , total_loss: 0.6536, data_loss: 0.6536\n",
      "step 600 , total_loss: 0.6515, data_loss: 0.6515\n",
      "step 620 , total_loss: 0.7021, data_loss: 0.7021\n",
      "step 640 , total_loss: 0.6494, data_loss: 0.6494\n",
      "step 660 , total_loss: 0.6427, data_loss: 0.6427\n",
      "step 680 , total_loss: 0.6415, data_loss: 0.6415\n",
      "step 700 , total_loss: 0.5334, data_loss: 0.5334\n",
      "at epoch 5 train info: auc:0.7028, exp_var:0.1239047646522522, logloss:0.6228, mae:0.43836528, rmse:0.46561787, rsquare:0.12374883433993633 eval info: auc:0.664, exp_var:0.07932615280151367, logloss:0.6419, mae:0.44751272, rmse:0.47455242, rsquare:0.07550199125679302\n",
      "at epoch 5 , train time: 8.5 eval time: 6.1\n",
      "step 20 , total_loss: 0.5468, data_loss: 0.5468\n",
      "step 40 , total_loss: 0.6443, data_loss: 0.6443\n",
      "step 60 , total_loss: 0.6361, data_loss: 0.6361\n",
      "step 80 , total_loss: 0.6170, data_loss: 0.6170\n",
      "step 100 , total_loss: 0.6884, data_loss: 0.6884\n",
      "step 120 , total_loss: 0.6273, data_loss: 0.6273\n",
      "step 140 , total_loss: 0.6998, data_loss: 0.6998\n",
      "step 160 , total_loss: 0.6445, data_loss: 0.6445\n",
      "step 180 , total_loss: 0.6941, data_loss: 0.6941\n",
      "step 200 , total_loss: 0.7398, data_loss: 0.7398\n",
      "step 220 , total_loss: 0.5866, data_loss: 0.5866\n",
      "step 240 , total_loss: 0.5370, data_loss: 0.5370\n",
      "step 260 , total_loss: 0.5987, data_loss: 0.5987\n",
      "step 280 , total_loss: 0.5468, data_loss: 0.5468\n",
      "step 300 , total_loss: 0.6592, data_loss: 0.6592\n",
      "step 320 , total_loss: 0.5580, data_loss: 0.5580\n",
      "step 340 , total_loss: 0.6649, data_loss: 0.6649\n",
      "step 360 , total_loss: 0.5228, data_loss: 0.5228\n",
      "step 380 , total_loss: 0.6568, data_loss: 0.6568\n",
      "step 400 , total_loss: 0.6691, data_loss: 0.6691\n",
      "step 420 , total_loss: 0.7088, data_loss: 0.7088\n",
      "step 440 , total_loss: 0.7416, data_loss: 0.7416\n",
      "step 460 , total_loss: 0.6160, data_loss: 0.6160\n",
      "step 480 , total_loss: 0.6615, data_loss: 0.6615\n",
      "step 500 , total_loss: 0.5888, data_loss: 0.5888\n",
      "step 520 , total_loss: 0.5214, data_loss: 0.5214\n",
      "step 540 , total_loss: 0.7857, data_loss: 0.7857\n",
      "step 560 , total_loss: 0.6769, data_loss: 0.6769\n",
      "step 580 , total_loss: 0.6496, data_loss: 0.6496\n",
      "step 600 , total_loss: 0.6523, data_loss: 0.6523\n",
      "step 620 , total_loss: 0.7010, data_loss: 0.7010\n",
      "step 640 , total_loss: 0.6471, data_loss: 0.6471\n",
      "step 660 , total_loss: 0.6421, data_loss: 0.6421\n",
      "step 680 , total_loss: 0.6442, data_loss: 0.6442\n",
      "step 700 , total_loss: 0.5228, data_loss: 0.5228\n",
      "at epoch 6 train info: auc:0.7059, exp_var:0.1281975507736206, logloss:0.6203, mae:0.43617207, rmse:0.46443513, rsquare:0.12803997750177776 eval info: auc:0.6662, exp_var:0.08130300045013428, logloss:0.6407, mae:0.44596636, rmse:0.4740253, rsquare:0.07768427600895922\n",
      "at epoch 6 , train time: 8.6 eval time: 6.2\n",
      "step 20 , total_loss: 0.5432, data_loss: 0.5432\n",
      "step 40 , total_loss: 0.6478, data_loss: 0.6478\n",
      "step 60 , total_loss: 0.6381, data_loss: 0.6381\n",
      "step 80 , total_loss: 0.5768, data_loss: 0.5768\n",
      "step 100 , total_loss: 0.6865, data_loss: 0.6865\n",
      "step 120 , total_loss: 0.6251, data_loss: 0.6251\n",
      "step 140 , total_loss: 0.6866, data_loss: 0.6866\n",
      "step 160 , total_loss: 0.6375, data_loss: 0.6375\n",
      "step 180 , total_loss: 0.6921, data_loss: 0.6921\n",
      "step 200 , total_loss: 0.7386, data_loss: 0.7386\n",
      "step 220 , total_loss: 0.5818, data_loss: 0.5818\n",
      "step 240 , total_loss: 0.5405, data_loss: 0.5405\n",
      "step 260 , total_loss: 0.5985, data_loss: 0.5985\n",
      "step 280 , total_loss: 0.5460, data_loss: 0.5460\n",
      "step 300 , total_loss: 0.6606, data_loss: 0.6606\n",
      "step 320 , total_loss: 0.5270, data_loss: 0.5270\n",
      "step 340 , total_loss: 0.6689, data_loss: 0.6689\n",
      "step 360 , total_loss: 0.5240, data_loss: 0.5240\n",
      "step 380 , total_loss: 0.6504, data_loss: 0.6504\n",
      "step 400 , total_loss: 0.6665, data_loss: 0.6665\n",
      "step 420 , total_loss: 0.7077, data_loss: 0.7077\n",
      "step 440 , total_loss: 0.7352, data_loss: 0.7352\n",
      "step 460 , total_loss: 0.6202, data_loss: 0.6202\n",
      "step 480 , total_loss: 0.6565, data_loss: 0.6565\n",
      "step 500 , total_loss: 0.5863, data_loss: 0.5863\n",
      "step 520 , total_loss: 0.5200, data_loss: 0.5200\n",
      "step 540 , total_loss: 0.7919, data_loss: 0.7919\n",
      "step 560 , total_loss: 0.6751, data_loss: 0.6751\n",
      "step 580 , total_loss: 0.6467, data_loss: 0.6467\n",
      "step 600 , total_loss: 0.6536, data_loss: 0.6536\n",
      "step 620 , total_loss: 0.6997, data_loss: 0.6997\n",
      "step 640 , total_loss: 0.6454, data_loss: 0.6454\n",
      "step 660 , total_loss: 0.6418, data_loss: 0.6418\n",
      "step 680 , total_loss: 0.6456, data_loss: 0.6456\n",
      "step 700 , total_loss: 0.5160, data_loss: 0.5160\n",
      "at epoch 7 train info: auc:0.7084, exp_var:0.1315745711326599, logloss:0.6184, mae:0.4344174, rmse:0.46357307, rsquare:0.1314318834182966 eval info: auc:0.6679, exp_var:0.08277356624603271, logloss:0.6399, mae:0.444746, rmse:0.47360322, rsquare:0.07931288934810932\n",
      "at epoch 7 , train time: 8.4 eval time: 6.2\n",
      "step 20 , total_loss: 0.5410, data_loss: 0.5410\n",
      "step 40 , total_loss: 0.6493, data_loss: 0.6493\n",
      "step 60 , total_loss: 0.6394, data_loss: 0.6394\n",
      "step 80 , total_loss: 0.5494, data_loss: 0.5494\n",
      "step 100 , total_loss: 0.6844, data_loss: 0.6844\n",
      "step 120 , total_loss: 0.6230, data_loss: 0.6230\n",
      "step 140 , total_loss: 0.6781, data_loss: 0.6781\n",
      "step 160 , total_loss: 0.6332, data_loss: 0.6332\n",
      "step 180 , total_loss: 0.6895, data_loss: 0.6895\n",
      "step 200 , total_loss: 0.7373, data_loss: 0.7373\n",
      "step 220 , total_loss: 0.5764, data_loss: 0.5764\n",
      "step 240 , total_loss: 0.5441, data_loss: 0.5441\n",
      "step 260 , total_loss: 0.5986, data_loss: 0.5986\n",
      "step 280 , total_loss: 0.5456, data_loss: 0.5456\n",
      "step 300 , total_loss: 0.6614, data_loss: 0.6614\n",
      "step 320 , total_loss: 0.5042, data_loss: 0.5042\n",
      "step 340 , total_loss: 0.6725, data_loss: 0.6725\n",
      "step 360 , total_loss: 0.5250, data_loss: 0.5250\n",
      "step 380 , total_loss: 0.6429, data_loss: 0.6429\n",
      "step 400 , total_loss: 0.6645, data_loss: 0.6645\n",
      "step 420 , total_loss: 0.7057, data_loss: 0.7057\n",
      "step 440 , total_loss: 0.7305, data_loss: 0.7305\n",
      "step 460 , total_loss: 0.6245, data_loss: 0.6245\n",
      "step 480 , total_loss: 0.6530, data_loss: 0.6530\n",
      "step 500 , total_loss: 0.5856, data_loss: 0.5856\n",
      "step 520 , total_loss: 0.5194, data_loss: 0.5194\n",
      "step 540 , total_loss: 0.7934, data_loss: 0.7934\n",
      "step 560 , total_loss: 0.6747, data_loss: 0.6747\n",
      "step 580 , total_loss: 0.6450, data_loss: 0.6450\n",
      "step 600 , total_loss: 0.6547, data_loss: 0.6547\n",
      "step 620 , total_loss: 0.6981, data_loss: 0.6981\n",
      "step 640 , total_loss: 0.6446, data_loss: 0.6446\n",
      "step 660 , total_loss: 0.6423, data_loss: 0.6423\n",
      "step 680 , total_loss: 0.6466, data_loss: 0.6466\n",
      "step 700 , total_loss: 0.5123, data_loss: 0.5123\n",
      "at epoch 8 train info: auc:0.7106, exp_var:0.13430637121200562, logloss:0.6169, mae:0.43297836, rmse:0.46281746, rsquare:0.1341883516599326 eval info: auc:0.6693, exp_var:0.08390474319458008, logloss:0.6393, mae:0.44374126, rmse:0.4732864, rsquare:0.08057078231212633\n",
      "at epoch 8 , train time: 8.4 eval time: 6.2\n",
      "step 20 , total_loss: 0.5387, data_loss: 0.5387\n",
      "step 40 , total_loss: 0.6486, data_loss: 0.6486\n",
      "step 60 , total_loss: 0.6399, data_loss: 0.6399\n",
      "step 80 , total_loss: 0.5298, data_loss: 0.5298\n",
      "step 100 , total_loss: 0.6837, data_loss: 0.6837\n",
      "step 120 , total_loss: 0.6220, data_loss: 0.6220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 140 , total_loss: 0.6729, data_loss: 0.6729\n",
      "step 160 , total_loss: 0.6329, data_loss: 0.6329\n",
      "step 180 , total_loss: 0.6859, data_loss: 0.6859\n",
      "step 200 , total_loss: 0.7361, data_loss: 0.7361\n",
      "step 220 , total_loss: 0.5721, data_loss: 0.5721\n",
      "step 240 , total_loss: 0.5464, data_loss: 0.5464\n",
      "step 260 , total_loss: 0.5985, data_loss: 0.5985\n",
      "step 280 , total_loss: 0.5458, data_loss: 0.5458\n",
      "step 300 , total_loss: 0.6616, data_loss: 0.6616\n",
      "step 320 , total_loss: 0.4879, data_loss: 0.4879\n",
      "step 340 , total_loss: 0.6758, data_loss: 0.6758\n",
      "step 360 , total_loss: 0.5256, data_loss: 0.5256\n",
      "step 380 , total_loss: 0.6367, data_loss: 0.6367\n",
      "step 400 , total_loss: 0.6630, data_loss: 0.6630\n",
      "step 420 , total_loss: 0.7034, data_loss: 0.7034\n",
      "step 440 , total_loss: 0.7279, data_loss: 0.7279\n",
      "step 460 , total_loss: 0.6288, data_loss: 0.6288\n",
      "step 480 , total_loss: 0.6498, data_loss: 0.6498\n",
      "step 500 , total_loss: 0.5864, data_loss: 0.5864\n",
      "step 520 , total_loss: 0.5195, data_loss: 0.5195\n",
      "step 540 , total_loss: 0.7924, data_loss: 0.7924\n",
      "step 560 , total_loss: 0.6748, data_loss: 0.6748\n",
      "step 580 , total_loss: 0.6443, data_loss: 0.6443\n",
      "step 600 , total_loss: 0.6549, data_loss: 0.6549\n",
      "step 620 , total_loss: 0.6965, data_loss: 0.6965\n",
      "step 640 , total_loss: 0.6442, data_loss: 0.6442\n",
      "step 660 , total_loss: 0.6431, data_loss: 0.6431\n",
      "step 680 , total_loss: 0.6476, data_loss: 0.6476\n",
      "step 700 , total_loss: 0.5103, data_loss: 0.5103\n",
      "at epoch 9 train info: auc:0.7123, exp_var:0.1364988088607788, logloss:0.6156, mae:0.43180326, rmse:0.46227697, rsquare:0.13640004744327894 eval info: auc:0.6702, exp_var:0.08467316627502441, logloss:0.6389, mae:0.44293624, rmse:0.47307506, rsquare:0.08143720055670811\n",
      "at epoch 9 , train time: 8.5 eval time: 6.6\n",
      "step 20 , total_loss: 0.5370, data_loss: 0.5370\n",
      "step 40 , total_loss: 0.6471, data_loss: 0.6471\n",
      "step 60 , total_loss: 0.6399, data_loss: 0.6399\n",
      "step 80 , total_loss: 0.5163, data_loss: 0.5163\n",
      "step 100 , total_loss: 0.6839, data_loss: 0.6839\n",
      "step 120 , total_loss: 0.6224, data_loss: 0.6224\n",
      "step 140 , total_loss: 0.6693, data_loss: 0.6693\n",
      "step 160 , total_loss: 0.6373, data_loss: 0.6373\n",
      "step 180 , total_loss: 0.6827, data_loss: 0.6827\n",
      "step 200 , total_loss: 0.7357, data_loss: 0.7357\n",
      "step 220 , total_loss: 0.5690, data_loss: 0.5690\n",
      "step 240 , total_loss: 0.5473, data_loss: 0.5473\n",
      "step 260 , total_loss: 0.5987, data_loss: 0.5987\n",
      "step 280 , total_loss: 0.5463, data_loss: 0.5463\n",
      "step 300 , total_loss: 0.6618, data_loss: 0.6618\n",
      "step 320 , total_loss: 0.4763, data_loss: 0.4763\n",
      "step 340 , total_loss: 0.6786, data_loss: 0.6786\n",
      "step 360 , total_loss: 0.5260, data_loss: 0.5260\n",
      "step 380 , total_loss: 0.6312, data_loss: 0.6312\n",
      "step 400 , total_loss: 0.6616, data_loss: 0.6616\n",
      "step 420 , total_loss: 0.7014, data_loss: 0.7014\n",
      "step 440 , total_loss: 0.7262, data_loss: 0.7262\n",
      "step 460 , total_loss: 0.6325, data_loss: 0.6325\n",
      "step 480 , total_loss: 0.6472, data_loss: 0.6472\n",
      "step 500 , total_loss: 0.5877, data_loss: 0.5877\n",
      "step 520 , total_loss: 0.5205, data_loss: 0.5205\n",
      "step 540 , total_loss: 0.7907, data_loss: 0.7907\n",
      "step 560 , total_loss: 0.6747, data_loss: 0.6747\n",
      "step 580 , total_loss: 0.6445, data_loss: 0.6445\n",
      "step 600 , total_loss: 0.6548, data_loss: 0.6548\n",
      "step 620 , total_loss: 0.6954, data_loss: 0.6954\n",
      "step 640 , total_loss: 0.6440, data_loss: 0.6440\n",
      "step 660 , total_loss: 0.6441, data_loss: 0.6441\n",
      "step 680 , total_loss: 0.6486, data_loss: 0.6486\n",
      "step 700 , total_loss: 0.5090, data_loss: 0.5090\n",
      "at epoch 10 train info: auc:0.7136, exp_var:0.13829165697097778, logloss:0.6146, mae:0.4307758, rmse:0.46173584, rsquare:0.13821574999152908 eval info: auc:0.6709, exp_var:0.08514660596847534, logloss:0.6386, mae:0.4422313, rmse:0.4728636, rsquare:0.08204738177160353\n",
      "at epoch 10 , train time: 8.4 eval time: 6.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<reco_utils.recommender.deeprec.models.xDeepFM.XDeepFMModel at 0x7fc81978e748>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_file, valid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exp_var': -6.733170509338379, 'mae': 0.55921715, 'logloss': 0.9299, 'rmse': 0.5893216, 'rsquare': -66.95369215530899, 'auc': 0.5113}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "res_syn": {
        "auc": 0.5113,
        "exp_var": -6.733170509338379,
        "logloss": 0.9299,
        "mae": 0.5592171549797058,
        "rmse": 0.5893216133117676,
        "rsquare": -66.95369215530899
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_syn = model.run_eval(test_file)\n",
    "print(res_syn)\n",
    "pm.record(\"res_syn\", res_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of  top N recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_syn = model.run_eval_topN(test_file,hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\\[1\\] Lian, J., Zhou, X., Zhang, F., Chen, Z., Xie, X., & Sun, G. (2018). xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems.Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, KDD 2018, London, UK, August 19-23, 2018.<br>\n",
    "\\[2\\] The Criteo datasets: http://labs.criteo.com/category/dataset/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
