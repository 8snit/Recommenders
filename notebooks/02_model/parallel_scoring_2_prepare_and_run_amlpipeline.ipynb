{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The goal of this notebook is to create and run a parallel scoring pipeline. This is the third of three notebooks.\n",
    "\n",
    "The order of the execution of these notebooks should be:\n",
    "\n",
    "- [parallel_scoring_0_prepare_azure_resources](parallel_scoring_0_prepare_azure_resources.ipynb): This creates the azure resources used in other notebooks. This should already be run when this notebook is executed.\n",
    "- [parallel_scoring_1_prepare_data_and_model](parallel_scoring_1_prepare_data_and_model.ipynb): This uploads ratings data and an example model to Azure where agents can see them. This should already be run when this notebook is executed.\n",
    "- [parallel_scoring_2_prepare_and_run_amlpipeline](parallel_scoring_2_prepare_and_run_amlpipeline.ipynb) **(This notebook)**: This creates and executes a parallel pipeline that leverages the resources and files created in prior steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "from azureml.core import Experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the configuration data created in the initial notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = 'pipeline_config_programmatic.json'\n",
    "with open(pipeline_config) as f:\n",
    "    j = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign-in to the Azure machine learning workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SP authentication\n",
    "sp_auth = ServicePrincipalAuthentication(\n",
    "    tenant_id=j[\"sp_tenant\"], username=j[\"sp_client\"], password=j[\"sp_secret\"]\n",
    ")\n",
    "\n",
    "# AML workspace\n",
    "aml_ws = Workspace.get(\n",
    "    name=j[\"aml_work_space\"],\n",
    "    auth=sp_auth,\n",
    "    subscription_id=str(j[\"subscription_id\"]),\n",
    "    resource_group=j[\"resource_group_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data stores and references\n",
    "\n",
    "A `Datastore` is an object that references an store of some kind. In these cases, we are just using azure blob storage.\n",
    "\n",
    "A `DataReference` is an object that resolves to the mount-point where its corresponding named `Datastore` is attached on a compute agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline inputs, models, and outputs\n",
    "inputs_ds = Datastore.register_azure_blob_container(\n",
    "    aml_ws,\n",
    "    datastore_name=\"inputs_ds\",\n",
    "    container_name=j[\"data_blob_container\"],\n",
    "    account_name=j[\"blob_account\"],\n",
    "    account_key=j[\"blob_key\"],\n",
    ")\n",
    "inputs_dir = DataReference(datastore=inputs_ds, data_reference_name=\"inputs\")\n",
    "\n",
    "models_ds = Datastore.register_azure_blob_container(\n",
    "    aml_ws,\n",
    "    datastore_name=\"models_ds\",\n",
    "    container_name=j[\"models_blob_container\"],\n",
    "    account_name=j[\"blob_account\"],\n",
    "    account_key=j[\"blob_key\"],\n",
    ")\n",
    "models_dir = DataReference(datastore=models_ds, data_reference_name=\"models\")\n",
    "\n",
    "outputs_ds = Datastore.register_azure_blob_container(\n",
    "    aml_ws,\n",
    "    datastore_name=\"outputs_ds\",\n",
    "    container_name=j[\"preds_blob_container\"],\n",
    "    account_name=j[\"blob_account\"],\n",
    "    account_key=j[\"blob_key\"],\n",
    ")\n",
    "outputs_dir = PipelineData(name=\"outputs\", datastore=outputs_ds, is_directory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the details of hte environment\n",
    "\n",
    "The `RunConfiguration` specifies the pre-requisites that need to be installed on the compute agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run config\n",
    "conda_dependencies = CondaDependencies.create(\n",
    "    pip_packages=j[\"pip_packages\"],\n",
    "    conda_packages=j[\"conda_packages\"],\n",
    "    python_version=j[\"python_version\"]\n",
    ")\n",
    "run_config = RunConfiguration(conda_dependencies=conda_dependencies)\n",
    "run_config.environment.docker.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some parameters for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIELENS_DATA_SIZE = '10m'\n",
    "\n",
    "if MOVIELENS_DATA_SIZE == '10m':\n",
    "    MAX_ALL = 72000\n",
    "    NUM_PER_RUN = 10000\n",
    "    compute_target = AmlCompute(aml_ws, j[\"cluster_name\"])    \n",
    "#    compute_target = AmlCompute(aml_ws, \"top10-mvl-d4v2\")    \n",
    "else:\n",
    "    MAX_ALL = 140000\n",
    "    NUM_PER_RUN = 10000\n",
    "    # getting memory errors...\n",
    "    compute_target = AmlCompute(aml_ws, \"top10-mvl-d4v2\")    \n",
    "\n",
    "# AML compute target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add one dependency\n",
    "\n",
    "The scoring script depends on the `reco_utils` module, because the scoring script needs to know about the `SARSingleNode` class, which is defined there. For that module to be loaded on the compute agents, it must be inside the directory that is referenced by the `source_directory` parameter of the `PythonScriptStep()`. So, we need to put a copy there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## copy reco_utils into scripts for this...\n",
    "reco_utils_home = '../../reco_utils'\n",
    "pipeline_reco_utils_copy = os.path.join(j[\"python_script_directory\"],'reco_utils')\n",
    "\n",
    "if os.path.exists(pipeline_reco_utils_copy):\n",
    "    ## if it already exists, then remove it to make sure nothing is cached incorrectly\n",
    "    print('removing stale copied version')\n",
    "    shutil.rmtree(pipeline_reco_utils_copy)\n",
    "\n",
    "## copy it over.\n",
    "shutil.copytree(reco_utils_home, pipeline_reco_utils_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the AML Pipeline\n",
    "\n",
    "Create a `PythonScriptStep` for each of the chunks in your dataset. The size of your chunk is determined by `NUM_PER_RUN` set above.\n",
    "\n",
    "Because there are no data dependencies (i.e. that the outputs of one step don't provide inputs to others), the steps will be queued in parallel and executed as resources become free.\n",
    "\n",
    "Each step contains information on:\n",
    "\n",
    "- `name`: the name of the step.\n",
    "- `script_name`: the name of the script to run\n",
    "- `arguments`: arguments passed to the script\n",
    "- `inputs`: inputs in the form of DataReferences. What blobs or datastores need to get mounted on the compute agent?\n",
    "- `outputs`: outputs in the form of DataReferences. What blobs or datastores does this step write to?\n",
    "- `source_directory`: What is the lowest point the directory tree that contains the script and all relevant dependencies. If there are modules that `script_name` depends on, then these should live inside the `source_directory`.\n",
    "- `compute_target`: What computational engine should be used?\n",
    "- `runconfig`: How does that computational engine need to be configured in order to run `script_name`? What dependencies must be installed?\n",
    "- `allow_reuse`: a flag to indicate whether it can re-use previously computed steps that have not had any changes. `False` indicates that a new run will always be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline step for a subset of data...\n",
    "\n",
    "steps = []\n",
    "CUR_MIN = 1\n",
    "CUR_MAX = CUR_MIN + NUM_PER_RUN\n",
    "\n",
    "## will say for 10m\n",
    "## if have copied the reco_utils dir to this dir...\n",
    "while CUR_MIN < MAX_ALL:\n",
    "    outputs_dir = PipelineData(name=\"outputs\", datastore=outputs_ds, is_directory=True)\n",
    "    cur_name = \"{}_{}_{}\".format(CUR_MIN, CUR_MAX, MOVIELENS_DATA_SIZE)\n",
    "    print(cur_name)\n",
    "    step = PythonScriptStep(\n",
    "        name=cur_name,\n",
    "        script_name=j[\"python_script_name\"],\n",
    "        arguments=[CUR_MIN, CUR_MAX, inputs_dir, models_dir, outputs_dir, '10', MOVIELENS_DATA_SIZE],\n",
    "        inputs=[models_dir, inputs_dir],\n",
    "        outputs=[outputs_dir],\n",
    "        source_directory=j[\"python_script_directory\"],\n",
    "        compute_target=compute_target,\n",
    "        runconfig=run_config,\n",
    "        allow_reuse=False,\n",
    "    )\n",
    "    steps.append(step)\n",
    "    CUR_MIN = CUR_MAX\n",
    "    CUR_MAX = CUR_MIN + NUM_PER_RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Validate the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=aml_ws, steps=steps)\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Pipeline as an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'prog_reco_score_%s' %(MOVIELENS_DATA_SIZE)\n",
    "print(exp_name)\n",
    "pipeline_run = Experiment(aml_ws, exp_name).submit(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (recommenders)",
   "language": "python",
   "name": "recommenders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
